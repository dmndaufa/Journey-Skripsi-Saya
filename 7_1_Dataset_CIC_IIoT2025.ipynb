{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "SdpCZLa8nGCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                            accuracy_score, precision_recall_fscore_support,\n",
        "                            matthews_corrcoef)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ih9lNGYcnFh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35LaIGAyn_A8",
        "outputId": "21d8fbcd-4304-4ccc-efd3-b1ab79b9dfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tahapan 1: Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "u2-gNo1dmEML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Load Dataset**"
      ],
      "metadata": {
        "id": "6RaERZhrnnLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvPwYQaNmCMc",
        "outputId": "7ba25736-6bdb-46ab-ead3-378e8a080445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "  - Total rows: 685,671\n",
            "  - Total columns: 95\n",
            "  - Memory usage: 3994.50 MB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_parquet('/content/drive/My Drive/Dataset/CIC_IIoT_2025/final_dataset.parquet')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"  - Total rows: {df.shape[0]:,}\")\n",
        "print(f\"  - Total columns: {df.shape[1]}\")\n",
        "print(f\"  - Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Drop Columns yang Tidak Perlu**"
      ],
      "metadata": {
        "id": "_TPosZLIouT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define columns to drop\n",
        "drop_columns = [\n",
        "    # Metadata (tidak predictive)\n",
        "    'device_name', 'device_mac',\n",
        "    'timestamp', 'timestamp_start', 'timestamp_end',\n",
        "\n",
        "    # Labels (keep label2 only sebagai target)\n",
        "    'label_full', 'label1', 'label3', 'label4',\n",
        "\n",
        "    # List columns (pakai count saja)\n",
        "    'log_data-types',\n",
        "    'network_ips_all', 'network_ips_dst', 'network_ips_src',\n",
        "    'network_macs_all', 'network_macs_dst', 'network_macs_src',\n",
        "    'network_ports_all', 'network_ports_dst', 'network_ports_src',\n",
        "    'network_protocols_all', 'network_protocols_dst', 'network_protocols_src'\n",
        "]"
      ],
      "metadata": {
        "id": "NRtUn3mBotmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify all columns exist before dropping\n",
        "existing_drop_cols = [col for col in drop_columns if col in df.columns]\n",
        "missing_drop_cols = [col for col in drop_columns if col not in df.columns]\n",
        "\n",
        "if missing_drop_cols:\n",
        "    print(f\"Warning: Columns not found in dataset: {missing_drop_cols}\")\n",
        "\n",
        "df = df.drop(columns=existing_drop_cols)\n",
        "\n",
        "print(f\"Dropped {len(existing_drop_cols)} columns\")\n",
        "print(f\"  - Remaining columns: {df.shape[1]}\")\n",
        "print(f\"  - Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y4ORxaOpAIs",
        "outputId": "2522f88c-191c-4d13-a48a-150e9fb9489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 22 columns\n",
            "  - Remaining columns: 73\n",
            "  - Memory usage: 412.21 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Mengkonversi Tipe Data ke Float32 & Int32 Memory Optimization**"
      ],
      "metadata": {
        "id": "QbnOxg9ApQC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "# Downcast integer columns (kecuali time_window dan label2)\n",
        "int_cols = df.select_dtypes(include=['int64']).columns.tolist()\n",
        "exclude_int = ['time_window', 'label2'] if 'label2' in int_cols else ['time_window']\n",
        "\n",
        "for col in int_cols:\n",
        "    if col not in exclude_int:\n",
        "        # Check if values fit in int32\n",
        "        col_min = df[col].min()\n",
        "        col_max = df[col].max()\n",
        "\n",
        "        if col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:\n",
        "            df[col] = df[col].astype('int32')\n",
        "        else:\n",
        "            print(f\"   {col}: Range too large for int32, keeping int64\")"
      ],
      "metadata": {
        "id": "Uh470H2yphJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downcast float columns\n",
        "float_cols = df.select_dtypes(include=['float64']).columns.tolist()\n",
        "\n",
        "for col in float_cols:\n",
        "    df[col] = df[col].astype('float32')\n",
        "\n",
        "final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
        "memory_saved = initial_memory - final_memory\n",
        "memory_reduction = (memory_saved / initial_memory) * 100"
      ],
      "metadata": {
        "id": "iY7cbANdpsOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory optimization complete!\")\n",
        "print(f\"  - Initial memory: {initial_memory:.2f} MB\")\n",
        "print(f\"  - Final memory: {final_memory:.2f} MB\")\n",
        "print(f\"  - Saved: {memory_saved:.2f} MB ({memory_reduction:.1f}% reduction)\")\n",
        "\n",
        "gc.collect()  # Force garbage collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bRmA9D_p1H0",
        "outputId": "ea7ab443-f7b7-4378-8c7c-335a8a70aef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory optimization complete!\n",
            "  - Initial memory: 412.21 MB\n",
            "  - Final memory: 226.50 MB\n",
            "  - Saved: 185.71 MB (45.1% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4 Cek Kualitas Data**"
      ],
      "metadata": {
        "id": "Qn0t34Zep-6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.1 Missing Values"
      ],
      "metadata": {
        "id": "dSOGRlKTm960"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum().sum()\n",
        "print(f\"  - Missing values: {missing_values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlNoGbbmqD-F",
        "outputId": "2c8d7efd-5b5e-43a3-c78d-328fa4ad8c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Missing values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.2 Infinite Values"
      ],
      "metadata": {
        "id": "eYXvGc1NnMmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for infinite values in float columns\n",
        "inf_count = 0\n",
        "float_cols = df.select_dtypes(include=['float32']).columns\n",
        "for col in float_cols:\n",
        "    inf_count += np.isinf(df[col]).sum()\n",
        "\n",
        "print(f\"  - Infinite values: {inf_count}\")\n",
        "\n",
        "if inf_count > 0:\n",
        "    print(f\"Replacing infinite values with NaN...\")\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Fill NaN with median\n",
        "    for col in float_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "    print(f\"Infinite values handled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxuzbZUZqJY8",
        "outputId": "01e5675e-b8bb-4bfe-9cd7-d1ad6d67bae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Infinite values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.3 Remove All-Zero Rows"
      ],
      "metadata": {
        "id": "f4d6JCU9nXiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [col for col in df.columns if col not in ['label2', 'time_window']]\n",
        "all_zero_mask = (df[feature_cols] == 0).all(axis=1)\n",
        "zero_count = all_zero_mask.sum()\n",
        "\n",
        "print(f\"    Found {zero_count:,} all-zero rows ({(zero_count/len(df)*100):.1f}%)\")\n",
        "\n",
        "if zero_count > 0:\n",
        "    print(f\"    Distribution of invalid rows:\")\n",
        "    for label, count in df[all_zero_mask]['label2'].value_counts().items():\n",
        "        pct = (count / zero_count) * 100\n",
        "        print(f\"      {label:12s}: {count:7,} ({pct:5.1f}%)\")\n",
        "\n",
        "    df = df[~all_zero_mask].copy()\n",
        "    print(f\"     Removed {zero_count:,} invalid rows\")\n",
        "    print(f\"    New total: {len(df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S62BtpLYoKku",
        "outputId": "0542d3ed-422a-446f-d036-49068a37bfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Found 136,798 all-zero rows (20.0%)\n",
            "    Distribution of invalid rows:\n",
            "      benign      : 121,090 ( 88.5%)\n",
            "      recon       :   8,998 (  6.6%)\n",
            "      ddos        :   2,655 (  1.9%)\n",
            "      dos         :   1,325 (  1.0%)\n",
            "      web         :     792 (  0.6%)\n",
            "      mitm        :     745 (  0.5%)\n",
            "      bruteforce  :     670 (  0.5%)\n",
            "      malware     :     523 (  0.4%)\n",
            "     Removed 136,798 invalid rows\n",
            "    New total: 548,873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.4 Remove Exact Duplicates (same time_window)"
      ],
      "metadata": {
        "id": "5oOoFpHtno1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols_with_tw = [col for col in df.columns if col not in ['label2']]\n",
        "initial_size = len(df)\n",
        "\n",
        "df = df.drop_duplicates(subset=feature_cols_with_tw, keep='first')\n",
        "\n",
        "dup_removed = initial_size - len(df)\n",
        "print(f\"    Removed {dup_removed:,} exact duplicates\")\n",
        "print(f\"    Final size: {len(df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-apx0VouYW",
        "outputId": "ad28a3f8-44f4-4aa5-e837-e2eb29e4b4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Removed 2,231 exact duplicates\n",
            "    Final size: 546,642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.5 Final Class Distribution"
      ],
      "metadata": {
        "id": "eB9J6ebTnfaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final class distribution (after cleaning):\")\n",
        "for label in sorted(df['label2'].unique()):\n",
        "    count = (df['label2'] == label).sum()\n",
        "    pct = (count / len(df)) * 100\n",
        "    print(f\"    {label:12s}: {count:7,} ({pct:5.2f}%)\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpCxDj1vqSlV",
        "outputId": "90299237-31cd-4972-b4a0-f7e6cc14c7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final class distribution (after cleaning):\n",
            "    benign      : 278,063 (50.87%)\n",
            "    bruteforce  :   5,328 ( 0.97%)\n",
            "    ddos        :  53,957 ( 9.87%)\n",
            "    dos         :  56,368 (10.31%)\n",
            "    malware     :  23,641 ( 4.32%)\n",
            "    mitm        :  24,448 ( 4.47%)\n",
            "    recon       :  96,625 (17.68%)\n",
            "    web         :   8,212 ( 1.50%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.5 Train/Test Split (Time-Based)**"
      ],
      "metadata": {
        "id": "k5GhW1aKqfkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time-based split untuk prevent temporal leakage\n",
        "train_windows = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "test_windows = [9, 10]\n",
        "\n",
        "train_df = df[df['time_window'].isin(train_windows)].copy()\n",
        "test_df = df[df['time_window'].isin(test_windows)].copy()\n",
        "\n",
        "print(f\"Train & test split complete!\")\n",
        "print(f\"  - Train set: {train_df.shape[0]:,} rows ({(len(train_df)/len(df)*100):.1f}%)\")\n",
        "print(f\"  - Test set:  {test_df.shape[0]:,} rows ({(len(test_df)/len(df)*100):.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l991u6a6qmXt",
        "outputId": "4390f272-a20b-4db4-a0b8-b5e5a6ede02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train & test split complete!\n",
            "  - Train set: 486,126 rows (88.9%)\n",
            "  - Test set:  60,516 rows (11.1%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify distributions\n",
        "print(f\"Train distribution:\")\n",
        "for label in sorted(train_df['label2'].unique()):\n",
        "    count = (train_df['label2'] == label).sum()\n",
        "    pct = (count / len(train_df)) * 100\n",
        "    print(f\"    {label:12s}: {count:7,} ({pct:5.2f}%)\")\n",
        "\n",
        "print(f\"\\nTest distribution:\")\n",
        "for label in sorted(test_df['label2'].unique()):\n",
        "    count = (test_df['label2'] == label).sum()\n",
        "    pct = (count / len(test_df)) * 100\n",
        "    print(f\"    {label:12s}: {count:7,} ({pct:5.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "612LM4s3q6hm",
        "outputId": "947739c3-35b6-4379-c677-9e7b45808110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train distribution:\n",
            "    benign      : 250,160 (51.46%)\n",
            "    bruteforce  :   4,619 ( 0.95%)\n",
            "    ddos        :  47,529 ( 9.78%)\n",
            "    dos         :  49,841 (10.25%)\n",
            "    malware     :  20,747 ( 4.27%)\n",
            "    mitm        :  21,493 ( 4.42%)\n",
            "    recon       :  84,591 (17.40%)\n",
            "    web         :   7,146 ( 1.47%)\n",
            "\n",
            "Test distribution:\n",
            "    benign      :  27,903 (46.11%)\n",
            "    bruteforce  :     709 ( 1.17%)\n",
            "    ddos        :   6,428 (10.62%)\n",
            "    dos         :   6,527 (10.79%)\n",
            "    malware     :   2,894 ( 4.78%)\n",
            "    mitm        :   2,955 ( 4.88%)\n",
            "    recon       :  12,034 (19.89%)\n",
            "    web         :   1,066 ( 1.76%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleanup\n",
        "del df\n",
        "gc.collect()\n",
        "\n",
        "print(\"Tahapan 1 Complete - Data is CLEAN and SPLIT!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci6w22PcpmbG",
        "outputId": "852df24e-7ad6-4e02-fc2c-f40602a16df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tahapan 1 Complete - Data is CLEAN and SPLIT!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "wQIyq5RS8-FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tahapan 2: Exploratory Data Analysis (EDA) ##"
      ],
      "metadata": {
        "id": "CaDz-g9kqS0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Class Distribution Analysis**"
      ],
      "metadata": {
        "id": "fvsk_TdtqhSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution(train_df, test_df, save_path='class_distribution.png'):\n",
        "    \"\"\"Plot class distribution for train and test sets\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Train distribution\n",
        "    train_counts = train_df['label2'].value_counts().sort_index()\n",
        "    axes[0].bar(range(len(train_counts)), train_counts.values,\n",
        "                color='steelblue', edgecolor='black')\n",
        "    axes[0].set_xticks(range(len(train_counts)))\n",
        "    axes[0].set_xticklabels(train_counts.index, rotation=45, ha='right')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].set_title('Train Set Class Distribution')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add percentage labels\n",
        "    for i, v in enumerate(train_counts.values):\n",
        "        pct = (v / len(train_df)) * 100\n",
        "        axes[0].text(i, v, f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Test distribution\n",
        "    test_counts = test_df['label2'].value_counts().sort_index()\n",
        "    axes[1].bar(range(len(test_counts)), test_counts.values,\n",
        "                color='coral', edgecolor='black')\n",
        "    axes[1].set_xticks(range(len(test_counts)))\n",
        "    axes[1].set_xticklabels(test_counts.index, rotation=45, ha='right')\n",
        "    axes[1].set_ylabel('Count')\n",
        "    axes[1].set_title('Test Set Class Distribution')\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add percentage labels\n",
        "    for i, v in enumerate(test_counts.values):\n",
        "        pct = (v / len(test_df)) * 100\n",
        "        axes[1].text(i, v, f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"   Saved: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # Print imbalance ratio\n",
        "    print(\"\\n   Imbalance Ratios (majority:minority):\")\n",
        "    max_count = train_counts.max()\n",
        "    for label, count in train_counts.items():\n",
        "        ratio = max_count / count\n",
        "        print(f\"      {label:12s}: 1:{ratio:.2f}\")"
      ],
      "metadata": {
        "id": "PzvPV5o280Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call function\n",
        "plot_class_distribution(train_df, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxLCVdWG9QD3",
        "outputId": "e8b6ef20-7ea8-48f5-f10b-8ad95dc8cbb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Saved: class_distribution.png\n",
            "\n",
            "   Imbalance Ratios (majority:minority):\n",
            "      benign      : 1:1.00\n",
            "      bruteforce  : 1:54.16\n",
            "      ddos        : 1:5.26\n",
            "      dos         : 1:5.02\n",
            "      malware     : 1:12.06\n",
            "      mitm        : 1:11.64\n",
            "      recon       : 1:2.96\n",
            "      web         : 1:35.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Feature Correlation**"
      ],
      "metadata": {
        "id": "Nic3ut2qsjnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_feature_correlation(train_df, top_n=20, save_path='feature_correlation.png'):\n",
        "    \"\"\"Analyze correlation between features\"\"\"\n",
        "    # Get numeric features only\n",
        "    feature_cols = [col for col in train_df.columns\n",
        "                   if col not in ['label2', 'time_window']]\n",
        "\n",
        "    # Sample for memory efficiency (10k rows)\n",
        "    sample_df = train_df[feature_cols].sample(n=min(10000, len(train_df)),\n",
        "                                               random_state=42)\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = sample_df.corr()\n",
        "\n",
        "    # Get top correlated pairs\n",
        "    corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            corr_pairs.append((\n",
        "                corr_matrix.columns[i],\n",
        "                corr_matrix.columns[j],\n",
        "                abs(corr_matrix.iloc[i, j])\n",
        "            ))\n",
        "\n",
        "    # Sort by correlation\n",
        "    corr_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(f\"Top {top_n} Highly Correlated Feature Pairs:\")\n",
        "    for feat1, feat2, corr_val in corr_pairs[:top_n]:\n",
        "        print(f\"      {feat1:40s} <-> {feat2:40s}: {corr_val:.3f}\")\n",
        "\n",
        "    # Plot top correlations\n",
        "    top_features = set()\n",
        "    for feat1, feat2, _ in corr_pairs[:top_n]:\n",
        "        top_features.add(feat1)\n",
        "        top_features.add(feat2)\n",
        "\n",
        "    top_features = list(top_features)[:30]  # Limit to 30 features\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    sns.heatmap(corr_matrix.loc[top_features, top_features],\n",
        "                cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=0.5,\n",
        "                cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title(f'Feature Correlation Heatmap (Top {len(top_features)} Features)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"   Saved: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    del sample_df, corr_matrix\n",
        "    gc.collect()\n",
        "\n",
        "# Call function\n",
        "analyze_feature_correlation(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "632yV2af9hMP",
        "outputId": "7477e7eb-52d5-4032-f634-24b139769c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 Highly Correlated Feature Pairs:\n",
            "      network_macs_dst_count                   <-> network_macs_src_count                  : 1.000\n",
            "      network_header-length_avg                <-> network_header-length_min               : 1.000\n",
            "      network_header-length_avg                <-> network_header-length_max               : 1.000\n",
            "      network_header-length_max                <-> network_header-length_min               : 1.000\n",
            "      network_ip-length_max                    <-> network_packet-size_max                 : 0.999\n",
            "      network_ips_all_count                    <-> network_ips_dst_count                   : 0.999\n",
            "      network_mss_avg                          <-> network_mss_max                         : 0.999\n",
            "      log_data-ranges_avg                      <-> log_data-ranges_max                     : 0.998\n",
            "      network_ip-length_max                    <-> network_payload-length_max              : 0.997\n",
            "      network_packet-size_max                  <-> network_payload-length_max              : 0.997\n",
            "      network_packets_all_count                <-> network_packets_dst_count               : 0.993\n",
            "      network_mss_avg                          <-> network_mss_min                         : 0.992\n",
            "      network_macs_all_count                   <-> network_macs_dst_count                  : 0.991\n",
            "      network_macs_all_count                   <-> network_macs_src_count                  : 0.991\n",
            "      network_mss_max                          <-> network_mss_min                         : 0.990\n",
            "      network_ips_src_count                    <-> network_macs_dst_count                  : 0.989\n",
            "      network_ips_src_count                    <-> network_macs_src_count                  : 0.989\n",
            "      network_tcp-flags-fin_count              <-> network_tcp-flags-rst_count             : 0.988\n",
            "      network_ips_src_count                    <-> network_macs_all_count                  : 0.984\n",
            "      network_ip-length_std_deviation          <-> network_packet-size_std_deviation       : 0.980\n",
            "   Saved: feature_correlation.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Feature Group Statistics**"
      ],
      "metadata": {
        "id": "gVpQ1_ZNsweQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_feature_groups(train_df):\n",
        "    \"\"\"Analyze statistics per feature group\"\"\"\n",
        "\n",
        "    # Define feature groups based on CIC-IIoT-2025 dataset\n",
        "    feature_groups = {\n",
        "        'log_stats': [col for col in train_df.columns if col.startswith('log_')],\n",
        "        'packet_rate': [col for col in train_df.columns\n",
        "                       if 'interval' in col or 'packets_' in col and '_count' in col],\n",
        "        'size_length': [col for col in train_df.columns\n",
        "                       if any(x in col for x in ['length', 'size', 'mss'])],\n",
        "        'tcp_flags': [col for col in train_df.columns if 'tcp-flags' in col],\n",
        "        'ip_flags': [col for col in train_df.columns if 'ip-flags' in col],\n",
        "        'address_diversity': [col for col in train_df.columns\n",
        "                             if any(x in col for x in ['ips_', 'macs_'])],\n",
        "        'network_multiplexing': [col for col in train_df.columns\n",
        "                                if any(x in col for x in ['ports_', 'protocols_'])],\n",
        "        'timing_control': [col for col in train_df.columns\n",
        "                          if any(x in col for x in ['time-delta', 'ttl', 'window-size'])],\n",
        "        'fragmentation': [col for col in train_df.columns if 'fragment' in col]\n",
        "    }\n",
        "\n",
        "    print(\"Feature Group Summary:\")\n",
        "    group_stats = {}\n",
        "    for group_name, features in feature_groups.items():\n",
        "        features = [f for f in features if f in train_df.columns]\n",
        "        if len(features) > 0:\n",
        "            group_data = train_df[features]\n",
        "            stats = {\n",
        "                'count': len(features),\n",
        "                'mean_avg': group_data.mean().mean(),\n",
        "                'mean_std': group_data.std().mean(),\n",
        "                'missing_pct': (group_data.isnull().sum().sum() /\n",
        "                               (len(group_data) * len(features))) * 100\n",
        "            }\n",
        "            group_stats[group_name] = stats\n",
        "            print(f\"      {group_name:25s}: {stats['count']:2d} features | \"\n",
        "                  f\"Avg: {stats['mean_avg']:8.2f} | \"\n",
        "                  f\"Std: {stats['mean_std']:8.2f}\")\n",
        "\n",
        "    return feature_groups, group_stats\n",
        "\n",
        "# Call function\n",
        "feature_groups, group_stats = analyze_feature_groups(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC9VJFd8-NO3",
        "outputId": "486cd34f-e502-4ede-979a-9323ca2bd51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Group Summary:\n",
            "      log_stats                :  7 features | Avg:    56.17 | Std:   159.31\n",
            "      packet_rate              :  5 features | Avg: 11652.73 | Std: 44381.99\n",
            "      size_length              : 24 features | Avg:  2679.49 | Std:  3000.32\n",
            "      tcp_flags                : 10 features | Avg:  2221.34 | Std: 21153.05\n",
            "      ip_flags                 :  4 features | Avg:     0.66 | Std:     0.65\n",
            "      address_diversity        :  6 features | Avg:     5.24 | Std:    12.39\n",
            "      network_multiplexing     :  6 features | Avg:  1669.49 | Std:  5999.29\n",
            "      timing_control           : 12 features | Avg:  5114.65 | Std:  5514.52\n",
            "      fragmentation            :  2 features | Avg:   507.11 | Std:  2802.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Temporal Pattern Analysis**"
      ],
      "metadata": {
        "id": "wtsIgxE3s4YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_temporal_patterns(train_df, test_df, save_path='temporal_patterns.png'):\n",
        "    \"\"\"Analyze attack patterns across time windows\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "    # Train temporal distribution\n",
        "    train_temporal = train_df.groupby(['time_window', 'label2']).size().unstack(fill_value=0)\n",
        "    train_temporal.plot(kind='bar', stacked=True, ax=axes[0],\n",
        "                       colormap='tab10', edgecolor='black', linewidth=0.5)\n",
        "    axes[0].set_title('Train Set: Attack Distribution Across Time Windows')\n",
        "    axes[0].set_xlabel('Time Window')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].legend(title='Attack Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Test temporal distribution\n",
        "    test_temporal = test_df.groupby(['time_window', 'label2']).size().unstack(fill_value=0)\n",
        "    test_temporal.plot(kind='bar', stacked=True, ax=axes[1],\n",
        "                      colormap='tab10', edgecolor='black', linewidth=0.5)\n",
        "    axes[1].set_title('Test Set: Attack Distribution Across Time Windows')\n",
        "    axes[1].set_xlabel('Time Window')\n",
        "    axes[1].set_ylabel('Count')\n",
        "    axes[1].legend(title='Attack Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"   Saved: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # Print temporal statistics\n",
        "    print(\"\\n   Temporal Statistics:\")\n",
        "    print(f\"      Train windows: {sorted(train_df['time_window'].unique())}\")\n",
        "    print(f\"      Test windows:  {sorted(test_df['time_window'].unique())}\")\n",
        "\n",
        "    for label in sorted(train_df['label2'].unique()):\n",
        "        train_windows = train_df[train_df['label2'] == label]['time_window'].value_counts()\n",
        "        print(f\"\\n      {label:12s}:\")\n",
        "        print(f\"         Appears in {len(train_windows)} windows\")\n",
        "        print(f\"         Avg per window: {train_windows.mean():.0f}\")\n",
        "        print(f\"         Std: {train_windows.std():.0f}\")\n",
        "\n",
        "# Call function\n",
        "analyze_temporal_patterns(train_df, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEyrb7f7-b7Y",
        "outputId": "8ce76292-504d-48db-f3a4-5326a84816f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Saved: temporal_patterns.png\n",
            "\n",
            "   Temporal Statistics:\n",
            "      Train windows: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]\n",
            "      Test windows:  [np.int64(9), np.int64(10)]\n",
            "\n",
            "      benign      :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 31270\n",
            "         Std: 16776\n",
            "\n",
            "      bruteforce  :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 577\n",
            "         Std: 397\n",
            "\n",
            "      ddos        :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 5941\n",
            "         Std: 4701\n",
            "\n",
            "      dos         :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 6230\n",
            "         Std: 5038\n",
            "\n",
            "      malware     :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 2593\n",
            "         Std: 2030\n",
            "\n",
            "      mitm        :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 2687\n",
            "         Std: 2078\n",
            "\n",
            "      recon       :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 10574\n",
            "         Std: 7648\n",
            "\n",
            "      web         :\n",
            "         Appears in 8 windows\n",
            "         Avg per window: 893\n",
            "         Std: 666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tahapan 2 Complete!\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nkGFBmO-vJA",
        "outputId": "8f1ea317-6c99-480e-8e43-ea8fa846a374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tahapan 2 Complete!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21621"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tahapan 3: Feature Engineering"
      ],
      "metadata": {
        "id": "whwUHn-vs8KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Time Window Features**"
      ],
      "metadata": {
        "id": "1iMe37YTtRPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lightweight_temporal_features(df):\n",
        "    \"\"\"\n",
        "    Create lightweight temporal features\n",
        "    Only add 5 aggregate features to minimize overhead\n",
        "    \"\"\"\n",
        "    print(\"   Creating temporal aggregate features...\")\n",
        "\n",
        "    # Sort by time_window and label\n",
        "    df = df.sort_values(['label2', 'time_window']).reset_index(drop=True)\n",
        "\n",
        "    # Get numeric columns (exclude label and time_window)\n",
        "    numeric_cols = [col for col in df.select_dtypes(include=['float32', 'int32']).columns\n",
        "                   if col not in ['label2', 'time_window']]\n",
        "\n",
        "    # Create 5 aggregate features only\n",
        "    temporal_features = []\n",
        "\n",
        "    # 1. Trend indicator (3-window moving average)\n",
        "    df['trend_indicator'] = df.groupby('label2')[numeric_cols[0]]\\\n",
        "        .rolling(window=3, min_periods=1).mean()\\\n",
        "        .reset_index(0, drop=True).astype('float32')\n",
        "    temporal_features.append('trend_indicator')\n",
        "\n",
        "    # 2. Volatility indicator (3-window moving std)\n",
        "    df['volatility_indicator'] = df.groupby('label2')[numeric_cols[0]]\\\n",
        "        .rolling(window=3, min_periods=1).std()\\\n",
        "        .reset_index(0, drop=True).fillna(0).astype('float32')\n",
        "    temporal_features.append('volatility_indicator')\n",
        "\n",
        "    # 3-5. Rolling stats for top 3 important features (we'll use first 3 numeric)\n",
        "    for i, col in enumerate(numeric_cols[:3], 1):\n",
        "        feat_name = f'roll_mean_{i}'\n",
        "        df[feat_name] = df.groupby('label2')[col]\\\n",
        "            .rolling(window=3, min_periods=1).mean()\\\n",
        "            .reset_index(0, drop=True).astype('float32')\n",
        "        temporal_features.append(feat_name)\n",
        "\n",
        "    print(f\"   Created {len(temporal_features)} temporal features\")\n",
        "    print(f\"   New shape: {df.shape}\")\n",
        "\n",
        "    return df, temporal_features\n",
        "\n",
        "# Apply to train and test (OPTIONAL - uncomment if you want to use)\n",
        "# train_df, temporal_features = create_lightweight_temporal_features(train_df)\n",
        "# test_df, _ = create_lightweight_temporal_features(test_df)\n",
        "\n",
        "print(\"   [SKIPPED] Temporal features are optional for memory efficiency\")\n",
        "temporal_features = []  # Empty list if not used"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCvahYiQ-6-w",
        "outputId": "39f69dc5-3c24-4afd-a932-06a72a67f8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [SKIPPED] Temporal features are optional for memory efficiency\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Feature Scaling/Normalization**"
      ],
      "metadata": {
        "id": "6P1lhIUotZSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(train_df, test_df, exclude_cols=['label2', 'time_window']):\n",
        "    \"\"\"\n",
        "    Normalize features using StandardScaler\n",
        "    Fit on train, transform both train and test\n",
        "    \"\"\"\n",
        "    print(\"   Normalizing features...\")\n",
        "\n",
        "    # Get feature columns\n",
        "    feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "    # Initialize scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit on train\n",
        "    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
        "\n",
        "    # Transform test\n",
        "    test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "    print(f\"   Normalized {len(feature_cols)} features\")\n",
        "    print(f\"   Scaler mean: {scaler.mean_[:5]}\")  # Show first 5\n",
        "    print(f\"   Scaler std:  {scaler.scale_[:5]}\")\n",
        "\n",
        "    return train_df, test_df, scaler, feature_cols\n",
        "\n",
        "# Apply normalization\n",
        "train_df, test_df, scaler, feature_cols = normalize_features(train_df, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujz4qQQQ_LRA",
        "outputId": "80bc00a8-66a1-41ab-bf41-e4d9970df336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Normalizing features...\n",
            "   Normalized 71 features\n",
            "   Scaler mean: [75.64021009 77.50452975 72.73823885  1.5173976   0.38324426]\n",
            "   Scaler std:  [239.42648566 241.95263683 233.67551727  17.90566799   0.61987365]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Feature Validation**"
      ],
      "metadata": {
        "id": "9Xdmtosptg95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_features(train_df, test_df, feature_cols):\n",
        "    \"\"\"Validate features after engineering\"\"\"\n",
        "\n",
        "    print(\"   Post-engineering validation:\")\n",
        "\n",
        "    # Check for NaN\n",
        "    train_nan = train_df[feature_cols].isnull().sum().sum()\n",
        "    test_nan = test_df[feature_cols].isnull().sum().sum()\n",
        "    print(f\"      Train NaN: {train_nan}\")\n",
        "    print(f\"      Test NaN:  {test_nan}\")\n",
        "\n",
        "    # Check for Inf\n",
        "    train_inf = np.isinf(train_df[feature_cols].select_dtypes(include=['float32'])).sum().sum()\n",
        "    test_inf = np.isinf(test_df[feature_cols].select_dtypes(include=['float32'])).sum().sum()\n",
        "    print(f\"      Train Inf: {train_inf}\")\n",
        "    print(f\"      Test Inf:  {test_inf}\")\n",
        "\n",
        "    # Check value ranges\n",
        "    print(f\"\\n   Value ranges (first 5 features):\")\n",
        "    for col in feature_cols[:5]:\n",
        "        print(f\"      {col:40s}: [{train_df[col].min():.3f}, {train_df[col].max():.3f}]\")\n",
        "\n",
        "    print(f\"\\n   Features validated successfully\")\n",
        "\n",
        "# Call validation\n",
        "validate_features(train_df, test_df, feature_cols)"
      ],
      "metadata": {
        "id": "j7_IDgi5_aUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e51e921-5a09-40a9-dcc4-d58db4a18a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Post-engineering validation:\n",
            "      Train NaN: 0\n",
            "      Test NaN:  0\n",
            "      Train Inf: 0.0\n",
            "      Test Inf:  0.0\n",
            "\n",
            "   Value ranges (first 5 features):\n",
            "      log_data-ranges_avg                     : [-0.316, 8.037]\n",
            "      log_data-ranges_max                     : [-0.320, 7.946]\n",
            "      log_data-ranges_min                     : [-0.315, 8.248]\n",
            "      log_data-ranges_std_deviation           : [-0.085, 46.168]\n",
            "      log_data-types_count                    : [-0.618, 2.608]\n",
            "\n",
            "   Features validated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tahapan 3 Complete!\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "zQ106y42_i7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tahapan 4: GIR Calculation"
      ],
      "metadata": {
        "id": "u7sFvb9ut49Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Define Severity Weights**"
      ],
      "metadata": {
        "id": "hRz25xYktODZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IIoT severity weights based on impact analysis\n",
        "severity_weights = {\n",
        "    'malware': 5.0,      # Highest: Persistent threat, data exfiltration, Stuxnet-like\n",
        "    'dos': 4.5,          # Safety-critical: Can halt industrial operations\n",
        "    'mitm': 4.5,         # Data integrity: Sensor data manipulation\n",
        "    'ddos': 4.0,         # Infrastructure: Network availability\n",
        "    'bruteforce': 3.5,   # Credential: Lateral movement risk\n",
        "    'web': 3.0,          # Application-layer: Less critical in IIoT\n",
        "    'recon': 2.5,        # Pre-attack: Reconnaissance phase\n",
        "    'benign': 1.0        # Normal traffic\n",
        "}\n",
        "\n",
        "print(\"   IIoT Severity Weights (Higher = More Critical):\")\n",
        "for label, weight in sorted(severity_weights.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"      {label:12s}: {weight:.1f}\")"
      ],
      "metadata": {
        "id": "oH2EbURa_s3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89bf381-5754-4515-df13-84f6c15cb5e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   IIoT Severity Weights (Higher = More Critical):\n",
            "      malware     : 5.0\n",
            "      dos         : 4.5\n",
            "      mitm        : 4.5\n",
            "      ddos        : 4.0\n",
            "      bruteforce  : 3.5\n",
            "      web         : 3.0\n",
            "      recon       : 2.5\n",
            "      benign      : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Calculate GIR per Class**"
      ],
      "metadata": {
        "id": "gABK81JluJ8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_adaptive_gir(train_df, severity_weights):\n",
        "    \"\"\"\n",
        "    Calculate Adaptive GIR with severity weighting\n",
        "\n",
        "    Formula: GIR = (w_severity  n_maj) / (1.0  n_min)\n",
        "\n",
        "    Unlike FIGS original (fixed w_min=2), A-FIGS uses dynamic severity weights\n",
        "    \"\"\"\n",
        "    class_counts = train_df['label2'].value_counts()\n",
        "    n_maj = class_counts.max()  # Majority class count\n",
        "\n",
        "    gir_values = {}\n",
        "    for label in class_counts.index:\n",
        "        n_min = class_counts[label]\n",
        "        w_severity = severity_weights.get(label, 1.0)\n",
        "\n",
        "        # A-FIGS Formula\n",
        "        gir = (w_severity * n_maj) / (1.0 * n_min)\n",
        "        gir_values[label] = gir\n",
        "\n",
        "    return gir_values"
      ],
      "metadata": {
        "id": "VVg-a2fI_2cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate GIR\n",
        "gir_values = calculate_adaptive_gir(train_df, severity_weights)\n",
        "\n",
        "print(\"Adaptive GIR Values:\")\n",
        "for label, gir in sorted(gir_values.items(), key=lambda x: x[1], reverse=True):\n",
        "    count = (train_df['label2'] == label).sum()\n",
        "    print(f\"      {label:12s}: GIR={gir:8.2f} (n={count:7,}, w={severity_weights[label]:.1f})\")"
      ],
      "metadata": {
        "id": "N1knCqJ1_9dZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91813368-8f97-41f1-a3ef-67fcaf0d77ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adaptive GIR Values:\n",
            "      bruteforce  : GIR=  189.56 (n=  4,619, w=3.5)\n",
            "      web         : GIR=  105.02 (n=  7,146, w=3.0)\n",
            "      malware     : GIR=   60.29 (n= 20,747, w=5.0)\n",
            "      mitm        : GIR=   52.38 (n= 21,493, w=4.5)\n",
            "      dos         : GIR=   22.59 (n= 49,841, w=4.5)\n",
            "      ddos        : GIR=   21.05 (n= 47,529, w=4.0)\n",
            "      recon       : GIR=    7.39 (n= 84,591, w=2.5)\n",
            "      benign      : GIR=    1.00 (n=250,160, w=1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Categorize (Plentiful/Limited/Sparse)**"
      ],
      "metadata": {
        "id": "vR0TqmINuRQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_by_gir(gir_values):\n",
        "    \"\"\"\n",
        "    Categorize classes based on GIR percentiles:\n",
        "    - Plentiful: GIR < 33rd percentile\n",
        "    - Limited:   33rd <= GIR < 67th percentile\n",
        "    - Sparse:    GIR >= 67th percentile\n",
        "    \"\"\"\n",
        "    gir_list = list(gir_values.values())\n",
        "    p33 = np.percentile(gir_list, 33)\n",
        "    p67 = np.percentile(gir_list, 67)\n",
        "\n",
        "    categories = {}\n",
        "    for label, gir in gir_values.items():\n",
        "        if gir < p33:\n",
        "            categories[label] = 'Plentiful'\n",
        "        elif gir < p67:\n",
        "            categories[label] = 'Limited'\n",
        "        else:\n",
        "            categories[label] = 'Sparse'\n",
        "\n",
        "    return categories, p33, p67"
      ],
      "metadata": {
        "id": "Vp-xG3TsALgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorize\n",
        "categories, p33, p67 = categorize_by_gir(gir_values)\n",
        "\n",
        "print(f\"   GIR Percentiles:\")\n",
        "print(f\"      33rd percentile: {p33:.2f}\")\n",
        "print(f\"      67th percentile: {p67:.2f}\")\n",
        "\n",
        "print(f\"\\n   Class Categories:\")\n",
        "for category_name in ['Plentiful', 'Limited', 'Sparse']:\n",
        "    classes = [label for label, cat in categories.items() if cat == category_name]\n",
        "    print(f\"\\n      {category_name}:\")\n",
        "    for label in classes:\n",
        "        count = (train_df['label2'] == label).sum()\n",
        "        print(f\"         {label:12s} (n={count:7,}, GIR={gir_values[label]:6.2f})\")\n",
        "\n",
        "# Store for later use\n",
        "plentiful_classes = [label for label, cat in categories.items() if cat == 'Plentiful']\n",
        "limited_classes = [label for label, cat in categories.items() if cat == 'Limited']\n",
        "sparse_classes = [label for label, cat in categories.items() if cat == 'Sparse']"
      ],
      "metadata": {
        "id": "EIj-e6rYAO0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ed3b82-9b6a-4889-eb01-53949ba1f2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GIR Percentiles:\n",
            "      33rd percentile: 21.53\n",
            "      67th percentile: 57.84\n",
            "\n",
            "   Class Categories:\n",
            "\n",
            "      Plentiful:\n",
            "         benign       (n=250,160, GIR=  1.00)\n",
            "         recon        (n= 84,591, GIR=  7.39)\n",
            "         ddos         (n= 47,529, GIR= 21.05)\n",
            "\n",
            "      Limited:\n",
            "         dos          (n= 49,841, GIR= 22.59)\n",
            "         mitm         (n= 21,493, GIR= 52.38)\n",
            "\n",
            "      Sparse:\n",
            "         malware      (n= 20,747, GIR= 60.29)\n",
            "         web          (n=  7,146, GIR=105.02)\n",
            "         bruteforce   (n=  4,619, GIR=189.56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.4 Visualization**"
      ],
      "metadata": {
        "id": "D57-hqqruWYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_gir_analysis(gir_values, categories, severity_weights, save_path='gir_analysis.png'):\n",
        "    \"\"\"Visualize GIR analysis\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. GIR values bar plot\n",
        "    labels = list(gir_values.keys())\n",
        "    girs = [gir_values[label] for label in labels]\n",
        "    colors = ['green' if categories[label]=='Plentiful'\n",
        "              else 'orange' if categories[label]=='Limited'\n",
        "              else 'red' for label in labels]\n",
        "\n",
        "    axes[0, 0].barh(labels, girs, color=colors, edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('GIR Value')\n",
        "    axes[0, 0].set_title('Adaptive GIR per Class')\n",
        "    axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "    axes[0, 0].axvline(p33, color='blue', linestyle='--', label='33rd percentile')\n",
        "    axes[0, 0].axvline(p67, color='purple', linestyle='--', label='67th percentile')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # 2. Severity weights\n",
        "    weights = [severity_weights[label] for label in labels]\n",
        "    axes[0, 1].barh(labels, weights, color='steelblue', edgecolor='black')\n",
        "    axes[0, 1].set_xlabel('Severity Weight')\n",
        "    axes[0, 1].set_title('IIoT Severity Weights')\n",
        "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # 3. Category distribution\n",
        "    category_counts = {}\n",
        "    for cat in ['Plentiful', 'Limited', 'Sparse']:\n",
        "        category_counts[cat] = sum(1 for c in categories.values() if c == cat)\n",
        "\n",
        "    axes[1, 0].pie(category_counts.values(), labels=category_counts.keys(),\n",
        "                   autopct='%1.1f%%', colors=['green', 'orange', 'red'],\n",
        "                   startangle=90, explode=[0.05, 0.05, 0.05])\n",
        "    axes[1, 0].set_title('Class Category Distribution')\n",
        "\n",
        "    # 4. GIR vs Sample Count\n",
        "    counts = [len(train_df[train_df['label2']==label]) for label in labels]\n",
        "    scatter_colors = [colors[i] for i in range(len(labels))]\n",
        "    axes[1, 1].scatter(counts, girs, c=scatter_colors, s=150,\n",
        "                      edgecolors='black', alpha=0.7)\n",
        "    for i, label in enumerate(labels):\n",
        "        axes[1, 1].annotate(label, (counts[i], girs[i]),\n",
        "                           fontsize=9, ha='right')\n",
        "    axes[1, 1].set_xlabel('Sample Count')\n",
        "    axes[1, 1].set_ylabel('GIR Value')\n",
        "    axes[1, 1].set_title('GIR vs Sample Count')\n",
        "    axes[1, 1].grid(alpha=0.3)\n",
        "    axes[1, 1].set_xscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"   Saved: {save_path}\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "FBsStMdGAdax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot GIR analysis\n",
        "plot_gir_analysis(gir_values, categories, severity_weights)"
      ],
      "metadata": {
        "id": "oiuSX4cAArjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f2994f-390b-4be9-945f-1b622ba6c1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Saved: gir_analysis.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tahapan 4 Complete!\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "MWo-P5MYAs8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9aac5f-ca63-4cef-9455-ad3e902233ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tahapan 4 Complete!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "961"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "7jbB02CZ4hUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tahapan 5: Group-Based Feature Selection"
      ],
      "metadata": {
        "id": "jvRLoKy2tsOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.1 Define Feature Groups**"
      ],
      "metadata": {
        "id": "07pHPhRGu45Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_iiot_feature_groups(train_df):\n",
        "    \"\"\"\n",
        "    Define 9 IIoT-specific feature groups based on CIC-IIoT-2025\n",
        "    \"\"\"\n",
        "    all_cols = [col for col in train_df.columns\n",
        "                if col not in ['label2', 'time_window']]\n",
        "\n",
        "    feature_groups = {\n",
        "        'log_stats': [\n",
        "            col for col in all_cols\n",
        "            if col.startswith('log_') and 'interval' not in col\n",
        "        ],\n",
        "\n",
        "        'packet_rate': [\n",
        "            col for col in all_cols\n",
        "            if 'interval' in col or ('packets_' in col and '_count' in col)\n",
        "        ],\n",
        "\n",
        "        'size_length': [\n",
        "            col for col in all_cols\n",
        "            if any(x in col for x in ['length', 'size', 'mss', 'payload'])\n",
        "               and 'window-size' not in col\n",
        "        ],\n",
        "\n",
        "        'tcp_flags': [\n",
        "            col for col in all_cols if 'tcp-flags' in col\n",
        "        ],\n",
        "\n",
        "        'ip_flags': [\n",
        "            col for col in all_cols if 'ip-flags' in col\n",
        "        ],\n",
        "\n",
        "        'address_diversity': [\n",
        "            col for col in all_cols\n",
        "            if any(x in col for x in ['ips_', 'macs_']) and '_count' in col\n",
        "        ],\n",
        "\n",
        "        'network_multiplexing': [\n",
        "            col for col in all_cols\n",
        "            if any(x in col for x in ['ports_', 'protocols_']) and '_count' in col\n",
        "        ],\n",
        "\n",
        "        'timing_control': [\n",
        "            col for col in all_cols\n",
        "            if any(x in col for x in ['time-delta', 'ttl', 'window-size'])\n",
        "        ],\n",
        "\n",
        "        'fragmentation': [\n",
        "            col for col in all_cols if 'fragment' in col\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Print summary\n",
        "    print(\"Feature Groups Defined:\")\n",
        "    total_features = 0\n",
        "    for group_name, features in feature_groups.items():\n",
        "        print(f\"      {group_name:25s}: {len(features):2d} features\")\n",
        "        total_features += len(features)\n",
        "\n",
        "    print(f\"\\nTotal features: {total_features}\")\n",
        "\n",
        "    return feature_groups\n",
        "\n",
        "# Define groups\n",
        "feature_groups = define_iiot_feature_groups(train_df)"
      ],
      "metadata": {
        "id": "eoqxEEPg4q4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8086822a-08b4-4cec-cea0-35e5fc1acbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Groups Defined:\n",
            "      log_stats                :  6 features\n",
            "      packet_rate              :  5 features\n",
            "      size_length              : 20 features\n",
            "      tcp_flags                : 10 features\n",
            "      ip_flags                 :  4 features\n",
            "      address_diversity        :  6 features\n",
            "      network_multiplexing     :  6 features\n",
            "      timing_control           : 12 features\n",
            "      fragmentation            :  2 features\n",
            "\n",
            "Total features: 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define groups\n",
        "feature_groups = define_iiot_feature_groups(train_df)"
      ],
      "metadata": {
        "id": "yHh_-znZ45iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b642c336-6fd1-440d-9938-9c380082e7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Groups Defined:\n",
            "      log_stats                :  6 features\n",
            "      packet_rate              :  5 features\n",
            "      size_length              : 20 features\n",
            "      tcp_flags                : 10 features\n",
            "      ip_flags                 :  4 features\n",
            "      address_diversity        :  6 features\n",
            "      network_multiplexing     :  6 features\n",
            "      timing_control           : 12 features\n",
            "      fragmentation            :  2 features\n",
            "\n",
            "Total features: 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2 Sensitivity Analysis per Group**"
      ],
      "metadata": {
        "id": "rnrdFS0Rv_mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDiscriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight Discriminator for sensitivity analysis\n",
        "    Memory-efficient architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Fu3jrxGm5Qxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_discriminator_for_class(X_class, X_benign, epochs=20, batch_size=512):\n",
        "    \"\"\"\n",
        "    Train a simple discriminator to distinguish attack class from benign\n",
        "    Used for sensitivity analysis\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Prepare data\n",
        "    X_attack = torch.FloatTensor(X_class.values).to(device)\n",
        "    X_normal = torch.FloatTensor(X_benign.values).to(device)\n",
        "\n",
        "    # Labels: 1 for attack, 0 for benign\n",
        "    y_attack = torch.ones(len(X_attack), 1).to(device)\n",
        "    y_normal = torch.zeros(len(X_normal), 1).to(device)\n",
        "\n",
        "    # Combine\n",
        "    X_combined = torch.cat([X_attack, X_normal], dim=0)\n",
        "    y_combined = torch.cat([y_attack, y_normal], dim=0)\n",
        "\n",
        "    # Shuffle\n",
        "    indices = torch.randperm(len(X_combined))\n",
        "    X_combined = X_combined[indices]\n",
        "    y_combined = y_combined[indices]\n",
        "\n",
        "    # Create DataLoader\n",
        "    dataset = TensorDataset(X_combined, y_combined)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize discriminator\n",
        "    discriminator = SimpleDiscriminator(X_class.shape[1]).to(device)\n",
        "    optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Training loop\n",
        "    discriminator.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = discriminator(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"         Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "    return discriminator"
      ],
      "metadata": {
        "id": "rux0O76g5dGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sensitivity_analysis_per_group(discriminator, X_sample, feature_groups,\n",
        "                                   epsilon=1e-5, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform sensitivity analysis PER feature group\n",
        "    Returns top-K features from each group\n",
        "    \"\"\"\n",
        "    device = next(discriminator.parameters()).device\n",
        "    discriminator.eval()\n",
        "\n",
        "    selected_features = {}\n",
        "    all_importance_scores = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for group_name, group_features in feature_groups.items():\n",
        "            # Filter valid features\n",
        "            valid_features = [f for f in group_features if f in X_sample.columns]\n",
        "\n",
        "            if len(valid_features) == 0:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n      Analyzing {group_name} ({len(valid_features)} features)...\")\n",
        "\n",
        "            importance_scores = {}\n",
        "\n",
        "            # Get baseline output\n",
        "            X_tensor = torch.FloatTensor(X_sample.values).to(device)\n",
        "            baseline_output = discriminator(X_tensor).cpu().numpy()\n",
        "\n",
        "            # Perturb each feature\n",
        "            for feature in valid_features:\n",
        "                feature_idx = X_sample.columns.get_loc(feature)\n",
        "\n",
        "                # Create perturbed copy\n",
        "                X_perturbed = X_sample.copy()\n",
        "                X_perturbed.iloc[:, feature_idx] += epsilon\n",
        "\n",
        "                # Get perturbed output\n",
        "                X_pert_tensor = torch.FloatTensor(X_perturbed.values).to(device)\n",
        "                perturbed_output = discriminator(X_pert_tensor).cpu().numpy()\n",
        "\n",
        "                # Calculate importance\n",
        "                importance = np.abs(perturbed_output - baseline_output).mean()\n",
        "                importance_scores[feature] = importance\n",
        "\n",
        "            # Sort and select top-K\n",
        "            sorted_features = sorted(importance_scores.items(),\n",
        "                                   key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            top_features = [f[0] for f in sorted_features[:top_k]]\n",
        "            selected_features[group_name] = top_features\n",
        "            all_importance_scores[group_name] = importance_scores\n",
        "\n",
        "            print(f\"         Top-{top_k} features:\")\n",
        "            for i, (feat, score) in enumerate(sorted_features[:top_k], 1):\n",
        "                print(f\"            {i}. {feat:40s}: {score:.6f}\")\n",
        "\n",
        "    return selected_features, all_importance_scores"
      ],
      "metadata": {
        "id": "U7GeUQlB5xxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform group-based feature selection for each class category\n",
        "print(\"Starting sensitivity analysis...\")"
      ],
      "metadata": {
        "id": "93ha2JHG55Yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f772d53-199d-45ef-b61d-43ef63592f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting sensitivity analysis...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for efficiency (10k samples per class)\n",
        "sample_size = 10000\n",
        "X_benign_sample = train_df[train_df['label2'] == 'benign'][feature_cols].sample(\n",
        "    n=min(sample_size, len(train_df[train_df['label2'] == 'benign'])),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "selected_features_by_class = {}\n",
        "importance_scores_by_class = {}"
      ],
      "metadata": {
        "id": "BmUChtqe5_Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Limited and Sparse classes only (they need augmentation)\n",
        "target_classes = limited_classes + sparse_classes\n",
        "\n",
        "for attack_class in target_classes:\n",
        "    print(f\"Processing class: {attack_class}\")\n",
        "    print(f\"   Category: {categories[attack_class]}\")\n",
        "\n",
        "    # Get attack samples\n",
        "    X_attack = train_df[train_df['label2'] == attack_class][feature_cols]\n",
        "\n",
        "    if len(X_attack) < 100:\n",
        "        print(f\"      WARNING: Only {len(X_attack)} samples, using all\")\n",
        "        X_attack_sample = X_attack\n",
        "    else:\n",
        "        X_attack_sample = X_attack.sample(\n",
        "            n=min(sample_size, len(X_attack)),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    print(f\"      Training discriminator...\")\n",
        "    discriminator = train_discriminator_for_class(\n",
        "        X_attack_sample, X_benign_sample,\n",
        "        epochs=20, batch_size=256\n",
        "    )\n",
        "\n",
        "    print(f\"\\n      Running sensitivity analysis...\")\n",
        "    selected_feats, importance_scores = sensitivity_analysis_per_group(\n",
        "        discriminator, X_attack_sample, feature_groups,\n",
        "        epsilon=1e-5, top_k=5\n",
        "    )\n",
        "\n",
        "    selected_features_by_class[attack_class] = selected_feats\n",
        "    importance_scores_by_class[attack_class] = importance_scores\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del discriminator\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "ZrOLJ-RW6IgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e95db3c-b77c-46e4-c46d-7450477b89c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   Processing class: dos\n",
            "   Category: Limited\n",
            "      Training discriminator...\n",
            "         Epoch 10/20, Loss: 0.0558\n",
            "         Epoch 20/20, Loss: 0.0514\n",
            "\n",
            "      Running sensitivity analysis...\n",
            "\n",
            "      Analyzing log_stats (6 features)...\n",
            "         Top-5 features:\n",
            "            1. log_data-types_count                    : 0.000000\n",
            "            2. log_messages_count                      : 0.000000\n",
            "            3. log_data-ranges_std_deviation           : 0.000000\n",
            "            4. log_data-ranges_min                     : 0.000000\n",
            "            5. log_data-ranges_max                     : 0.000000\n",
            "\n",
            "      Analyzing packet_rate (5 features)...\n",
            "         Top-5 features:\n",
            "            1. network_packets_dst_count               : 0.000000\n",
            "            2. network_packets_all_count               : 0.000000\n",
            "            3. network_packets_src_count               : 0.000000\n",
            "            4. log_interval-messages                   : 0.000000\n",
            "            5. network_interval-packets                : 0.000000\n",
            "\n",
            "      Analyzing size_length (20 features)...\n",
            "         Top-5 features:\n",
            "            1. network_mss_std_deviation               : 0.000000\n",
            "            2. network_ip-length_min                   : 0.000000\n",
            "            3. network_payload-length_min              : 0.000000\n",
            "            4. network_payload-length_std_deviation    : 0.000000\n",
            "            5. network_mss_avg                         : 0.000000\n",
            "\n",
            "      Analyzing tcp_flags (10 features)...\n",
            "         Top-5 features:\n",
            "            1. network_tcp-flags-ack_count             : 0.000000\n",
            "            2. network_tcp-flags-psh_count             : 0.000000\n",
            "            3. network_tcp-flags-rst_count             : 0.000000\n",
            "            4. network_tcp-flags_min                   : 0.000000\n",
            "            5. network_tcp-flags_max                   : 0.000000\n",
            "\n",
            "      Analyzing ip_flags (4 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ip-flags_std_deviation          : 0.000000\n",
            "            2. network_ip-flags_avg                    : 0.000000\n",
            "            3. network_ip-flags_max                    : 0.000000\n",
            "            4. network_ip-flags_min                    : 0.000000\n",
            "\n",
            "      Analyzing address_diversity (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_macs_all_count                  : 0.000000\n",
            "            2. network_ips_src_count                   : 0.000000\n",
            "            3. network_ips_dst_count                   : 0.000000\n",
            "            4. network_macs_dst_count                  : 0.000000\n",
            "            5. network_ips_all_count                   : 0.000000\n",
            "\n",
            "      Analyzing network_multiplexing (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_protocols_src_count             : 0.000000\n",
            "            2. network_ports_dst_count                 : 0.000000\n",
            "            3. network_ports_all_count                 : 0.000000\n",
            "            4. network_ports_src_count                 : 0.000000\n",
            "            5. network_protocols_all_count             : 0.000000\n",
            "\n",
            "      Analyzing timing_control (12 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ttl_std_deviation               : 0.000000\n",
            "            2. network_time-delta_max                  : 0.000000\n",
            "            3. network_ttl_avg                         : 0.000000\n",
            "            4. network_ttl_max                         : 0.000000\n",
            "            5. network_window-size_avg                 : 0.000000\n",
            "\n",
            "      Analyzing fragmentation (2 features)...\n",
            "         Top-5 features:\n",
            "            1. network_fragmentation-score             : 0.000000\n",
            "            2. network_fragmented-packets              : 0.000000\n",
            "\n",
            "   Processing class: mitm\n",
            "   Category: Limited\n",
            "      Training discriminator...\n",
            "         Epoch 10/20, Loss: 0.1519\n",
            "         Epoch 20/20, Loss: 0.1080\n",
            "\n",
            "      Running sensitivity analysis...\n",
            "\n",
            "      Analyzing log_stats (6 features)...\n",
            "         Top-5 features:\n",
            "            1. log_data-types_count                    : 0.000000\n",
            "            2. log_messages_count                      : 0.000000\n",
            "            3. log_data-ranges_std_deviation           : 0.000000\n",
            "            4. log_data-ranges_avg                     : 0.000000\n",
            "            5. log_data-ranges_min                     : 0.000000\n",
            "\n",
            "      Analyzing packet_rate (5 features)...\n",
            "         Top-5 features:\n",
            "            1. network_packets_all_count               : 0.000001\n",
            "            2. network_packets_dst_count               : 0.000001\n",
            "            3. network_packets_src_count               : 0.000001\n",
            "            4. log_interval-messages                   : 0.000000\n",
            "            5. network_interval-packets                : 0.000000\n",
            "\n",
            "      Analyzing size_length (20 features)...\n",
            "         Top-5 features:\n",
            "            1. network_packet-size_min                 : 0.000004\n",
            "            2. network_ip-length_min                   : 0.000003\n",
            "            3. network_packet-size_avg                 : 0.000001\n",
            "            4. network_payload-length_avg              : 0.000001\n",
            "            5. network_payload-length_min              : 0.000001\n",
            "\n",
            "      Analyzing tcp_flags (10 features)...\n",
            "         Top-5 features:\n",
            "            1. network_tcp-flags_max                   : 0.000001\n",
            "            2. network_tcp-flags_std_deviation         : 0.000000\n",
            "            3. network_tcp-flags-ack_count             : 0.000000\n",
            "            4. network_tcp-flags-rst_count             : 0.000000\n",
            "            5. network_tcp-flags-syn_count             : 0.000000\n",
            "\n",
            "      Analyzing ip_flags (4 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ip-flags_std_deviation          : 0.000001\n",
            "            2. network_ip-flags_avg                    : 0.000001\n",
            "            3. network_ip-flags_min                    : 0.000000\n",
            "            4. network_ip-flags_max                    : 0.000000\n",
            "\n",
            "      Analyzing address_diversity (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_macs_all_count                  : 0.000000\n",
            "            2. network_ips_dst_count                   : 0.000000\n",
            "            3. network_ips_src_count                   : 0.000000\n",
            "            4. network_macs_dst_count                  : 0.000000\n",
            "            5. network_ips_all_count                   : 0.000000\n",
            "\n",
            "      Analyzing network_multiplexing (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ports_src_count                 : 0.000000\n",
            "            2. network_protocols_src_count             : 0.000000\n",
            "            3. network_protocols_all_count             : 0.000000\n",
            "            4. network_ports_all_count                 : 0.000000\n",
            "            5. network_protocols_dst_count             : 0.000000\n",
            "\n",
            "      Analyzing timing_control (12 features)...\n",
            "         Top-5 features:\n",
            "            1. network_window-size_min                 : 0.000004\n",
            "            2. network_window-size_std_deviation       : 0.000002\n",
            "            3. network_ttl_min                         : 0.000001\n",
            "            4. network_time-delta_min                  : 0.000001\n",
            "            5. network_ttl_avg                         : 0.000001\n",
            "\n",
            "      Analyzing fragmentation (2 features)...\n",
            "         Top-5 features:\n",
            "            1. network_fragmentation-score             : 0.000001\n",
            "            2. network_fragmented-packets              : 0.000000\n",
            "\n",
            "   Processing class: malware\n",
            "   Category: Sparse\n",
            "      Training discriminator...\n",
            "         Epoch 10/20, Loss: 0.1298\n",
            "         Epoch 20/20, Loss: 0.1176\n",
            "\n",
            "      Running sensitivity analysis...\n",
            "\n",
            "      Analyzing log_stats (6 features)...\n",
            "         Top-5 features:\n",
            "            1. log_data-ranges_avg                     : 0.000001\n",
            "            2. log_data-ranges_max                     : 0.000001\n",
            "            3. log_data-ranges_min                     : 0.000001\n",
            "            4. log_data-ranges_std_deviation           : 0.000000\n",
            "            5. log_data-types_count                    : 0.000000\n",
            "\n",
            "      Analyzing packet_rate (5 features)...\n",
            "         Top-5 features:\n",
            "            1. network_packets_src_count               : 0.000000\n",
            "            2. network_interval-packets                : 0.000000\n",
            "            3. network_packets_dst_count               : 0.000000\n",
            "            4. network_packets_all_count               : 0.000000\n",
            "            5. log_interval-messages                   : 0.000000\n",
            "\n",
            "      Analyzing size_length (20 features)...\n",
            "         Top-5 features:\n",
            "            1. network_packet-size_min                 : 0.000001\n",
            "            2. network_packet-size_avg                 : 0.000001\n",
            "            3. network_ip-length_avg                   : 0.000000\n",
            "            4. network_payload-length_std_deviation    : 0.000000\n",
            "            5. network_mss_std_deviation               : 0.000000\n",
            "\n",
            "      Analyzing tcp_flags (10 features)...\n",
            "         Top-5 features:\n",
            "            1. network_tcp-flags_max                   : 0.000001\n",
            "            2. network_tcp-flags_std_deviation         : 0.000000\n",
            "            3. network_tcp-flags-fin_count             : 0.000000\n",
            "            4. network_tcp-flags-ack_count             : 0.000000\n",
            "            5. network_tcp-flags-urg_count             : 0.000000\n",
            "\n",
            "      Analyzing ip_flags (4 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ip-flags_max                    : 0.000000\n",
            "            2. network_ip-flags_avg                    : 0.000000\n",
            "            3. network_ip-flags_min                    : 0.000000\n",
            "            4. network_ip-flags_std_deviation          : 0.000000\n",
            "\n",
            "      Analyzing address_diversity (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ips_all_count                   : 0.000000\n",
            "            2. network_ips_dst_count                   : 0.000000\n",
            "            3. network_macs_all_count                  : 0.000000\n",
            "            4. network_macs_dst_count                  : 0.000000\n",
            "            5. network_ips_src_count                   : 0.000000\n",
            "\n",
            "      Analyzing network_multiplexing (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ports_dst_count                 : 0.000000\n",
            "            2. network_ports_src_count                 : 0.000000\n",
            "            3. network_protocols_dst_count             : 0.000000\n",
            "            4. network_protocols_src_count             : 0.000000\n",
            "            5. network_ports_all_count                 : 0.000000\n",
            "\n",
            "      Analyzing timing_control (12 features)...\n",
            "         Top-5 features:\n",
            "            1. network_time-delta_avg                  : 0.000001\n",
            "            2. network_window-size_std_deviation       : 0.000000\n",
            "            3. network_time-delta_std_deviation        : 0.000000\n",
            "            4. network_window-size_avg                 : 0.000000\n",
            "            5. network_time-delta_min                  : 0.000000\n",
            "\n",
            "      Analyzing fragmentation (2 features)...\n",
            "         Top-5 features:\n",
            "            1. network_fragmented-packets              : 0.000000\n",
            "            2. network_fragmentation-score             : 0.000000\n",
            "\n",
            "   Processing class: web\n",
            "   Category: Sparse\n",
            "      Training discriminator...\n",
            "         Epoch 10/20, Loss: 0.0462\n",
            "         Epoch 20/20, Loss: 0.0365\n",
            "\n",
            "      Running sensitivity analysis...\n",
            "\n",
            "      Analyzing log_stats (6 features)...\n",
            "         Top-5 features:\n",
            "            1. log_data-types_count                    : 0.000000\n",
            "            2. log_data-ranges_max                     : 0.000000\n",
            "            3. log_data-ranges_avg                     : 0.000000\n",
            "            4. log_data-ranges_std_deviation           : 0.000000\n",
            "            5. log_data-ranges_min                     : 0.000000\n",
            "\n",
            "      Analyzing packet_rate (5 features)...\n",
            "         Top-5 features:\n",
            "            1. network_interval-packets                : 0.000000\n",
            "            2. network_packets_src_count               : 0.000000\n",
            "            3. network_packets_dst_count               : 0.000000\n",
            "            4. network_packets_all_count               : 0.000000\n",
            "            5. log_interval-messages                   : 0.000000\n",
            "\n",
            "      Analyzing size_length (20 features)...\n",
            "         Top-5 features:\n",
            "            1. network_packet-size_avg                 : 0.000000\n",
            "            2. network_payload-length_avg              : 0.000000\n",
            "            3. network_ip-length_avg                   : 0.000000\n",
            "            4. network_payload-length_min              : 0.000000\n",
            "            5. network_payload-length_std_deviation    : 0.000000\n",
            "\n",
            "      Analyzing tcp_flags (10 features)...\n",
            "         Top-5 features:\n",
            "            1. network_tcp-flags-syn_count             : 0.000000\n",
            "            2. network_tcp-flags_avg                   : 0.000000\n",
            "            3. network_tcp-flags-ack_count             : 0.000000\n",
            "            4. network_tcp-flags-fin_count             : 0.000000\n",
            "            5. network_tcp-flags-urg_count             : 0.000000\n",
            "\n",
            "      Analyzing ip_flags (4 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ip-flags_avg                    : 0.000000\n",
            "            2. network_ip-flags_std_deviation          : 0.000000\n",
            "            3. network_ip-flags_max                    : 0.000000\n",
            "            4. network_ip-flags_min                    : 0.000000\n",
            "\n",
            "      Analyzing address_diversity (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_macs_all_count                  : 0.000000\n",
            "            2. network_macs_dst_count                  : 0.000000\n",
            "            3. network_macs_src_count                  : 0.000000\n",
            "            4. network_ips_src_count                   : 0.000000\n",
            "            5. network_ips_dst_count                   : 0.000000\n",
            "\n",
            "      Analyzing network_multiplexing (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_protocols_src_count             : 0.000000\n",
            "            2. network_protocols_all_count             : 0.000000\n",
            "            3. network_ports_all_count                 : 0.000000\n",
            "            4. network_ports_dst_count                 : 0.000000\n",
            "            5. network_ports_src_count                 : 0.000000\n",
            "\n",
            "      Analyzing timing_control (12 features)...\n",
            "         Top-5 features:\n",
            "            1. network_time-delta_min                  : 0.000000\n",
            "            2. network_ttl_min                         : 0.000000\n",
            "            3. network_time-delta_std_deviation        : 0.000000\n",
            "            4. network_time-delta_avg                  : 0.000000\n",
            "            5. network_ttl_avg                         : 0.000000\n",
            "\n",
            "      Analyzing fragmentation (2 features)...\n",
            "         Top-5 features:\n",
            "            1. network_fragmented-packets              : 0.000000\n",
            "            2. network_fragmentation-score             : 0.000000\n",
            "\n",
            "   Processing class: bruteforce\n",
            "   Category: Sparse\n",
            "      Training discriminator...\n",
            "         Epoch 10/20, Loss: 0.1046\n",
            "         Epoch 20/20, Loss: 0.0918\n",
            "\n",
            "      Running sensitivity analysis...\n",
            "\n",
            "      Analyzing log_stats (6 features)...\n",
            "         Top-5 features:\n",
            "            1. log_data-types_count                    : 0.000001\n",
            "            2. log_messages_count                      : 0.000000\n",
            "            3. log_data-ranges_min                     : 0.000000\n",
            "            4. log_data-ranges_avg                     : 0.000000\n",
            "            5. log_data-ranges_std_deviation           : 0.000000\n",
            "\n",
            "      Analyzing packet_rate (5 features)...\n",
            "         Top-5 features:\n",
            "            1. network_interval-packets                : 0.000001\n",
            "            2. log_interval-messages                   : 0.000000\n",
            "            3. network_packets_all_count               : 0.000000\n",
            "            4. network_packets_src_count               : 0.000000\n",
            "            5. network_packets_dst_count               : 0.000000\n",
            "\n",
            "      Analyzing size_length (20 features)...\n",
            "         Top-5 features:\n",
            "            1. network_packet-size_min                 : 0.000001\n",
            "            2. network_payload-length_avg              : 0.000001\n",
            "            3. network_ip-length_std_deviation         : 0.000000\n",
            "            4. network_payload-length_std_deviation    : 0.000000\n",
            "            5. network_ip-length_avg                   : 0.000000\n",
            "\n",
            "      Analyzing tcp_flags (10 features)...\n",
            "         Top-5 features:\n",
            "            1. network_tcp-flags_min                   : 0.000000\n",
            "            2. network_tcp-flags_max                   : 0.000000\n",
            "            3. network_tcp-flags_std_deviation         : 0.000000\n",
            "            4. network_tcp-flags_avg                   : 0.000000\n",
            "            5. network_tcp-flags-syn_count             : 0.000000\n",
            "\n",
            "      Analyzing ip_flags (4 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ip-flags_avg                    : 0.000001\n",
            "            2. network_ip-flags_min                    : 0.000000\n",
            "            3. network_ip-flags_std_deviation          : 0.000000\n",
            "            4. network_ip-flags_max                    : 0.000000\n",
            "\n",
            "      Analyzing address_diversity (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_ips_dst_count                   : 0.000000\n",
            "            2. network_ips_all_count                   : 0.000000\n",
            "            3. network_ips_src_count                   : 0.000000\n",
            "            4. network_macs_src_count                  : 0.000000\n",
            "            5. network_macs_dst_count                  : 0.000000\n",
            "\n",
            "      Analyzing network_multiplexing (6 features)...\n",
            "         Top-5 features:\n",
            "            1. network_protocols_all_count             : 0.000000\n",
            "            2. network_protocols_src_count             : 0.000000\n",
            "            3. network_ports_dst_count                 : 0.000000\n",
            "            4. network_ports_all_count                 : 0.000000\n",
            "            5. network_protocols_dst_count             : 0.000000\n",
            "\n",
            "      Analyzing timing_control (12 features)...\n",
            "         Top-5 features:\n",
            "            1. network_window-size_std_deviation       : 0.000001\n",
            "            2. network_time-delta_min                  : 0.000001\n",
            "            3. network_ttl_avg                         : 0.000001\n",
            "            4. network_time-delta_avg                  : 0.000001\n",
            "            5. network_window-size_min                 : 0.000001\n",
            "\n",
            "      Analyzing fragmentation (2 features)...\n",
            "         Top-5 features:\n",
            "            1. network_fragmented-packets              : 0.000000\n",
            "            2. network_fragmentation-score             : 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.3 Select Top-K per Group**"
      ],
      "metadata": {
        "id": "vBk4H3gtwBYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_selected_features(selected_features_by_class):\n",
        "    \"\"\"\n",
        "    Consolidate selected features across all classes\n",
        "    \"\"\"\n",
        "    all_selected = set()\n",
        "\n",
        "    for attack_class, group_features in selected_features_by_class.items():\n",
        "        for group_name, features in group_features.items():\n",
        "            all_selected.update(features)\n",
        "\n",
        "    return list(all_selected)\n",
        "\n",
        "# Get all selected features\n",
        "selected_important_features = consolidate_selected_features(selected_features_by_class)\n",
        "\n",
        "print(f\"Total selected features: {len(selected_important_features)}\")\n",
        "print(f\"   Selected from original: {len(feature_cols)} features\")\n",
        "print(f\"   Reduction: {(1 - len(selected_important_features)/len(feature_cols))*100:.1f}%\")"
      ],
      "metadata": {
        "id": "q1Q4JTQI6ct4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe30d01c-91a2-45ed-bfff-faf7edf3b1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total selected features: 60\n",
            "   Selected from original: 71 features\n",
            "   Reduction: 15.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.4 Validate Selected Features**"
      ],
      "metadata": {
        "id": "UelMNYsdwDSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_selected_features(selected_features, feature_groups):\n",
        "    \"\"\"\n",
        "    Validate feature selection across groups\n",
        "    \"\"\"\n",
        "    print(\"Selected Features per Group:\")\n",
        "\n",
        "    group_distribution = {}\n",
        "    for group_name, group_feats in feature_groups.items():\n",
        "        selected_in_group = [f for f in selected_features if f in group_feats]\n",
        "        group_distribution[group_name] = len(selected_in_group)\n",
        "\n",
        "        if len(selected_in_group) > 0:\n",
        "            pct = (len(selected_in_group) / len(selected_features)) * 100\n",
        "            print(f\"      {group_name:25s}: {len(selected_in_group):2d} ({pct:5.1f}%)\")\n",
        "\n",
        "    return group_distribution\n",
        "\n",
        "# Validate\n",
        "group_distribution = validate_selected_features(\n",
        "    selected_important_features, feature_groups\n",
        ")\n",
        "\n",
        "print(\"\\n Tahapan 5 Complete!\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "lGs5NPts60Wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ada854-9526-4cc9-f1f1-0bfb7a93138c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features per Group:\n",
            "      log_stats                :  6 ( 10.0%)\n",
            "      packet_rate              :  5 (  8.3%)\n",
            "      size_length              : 10 ( 16.7%)\n",
            "      tcp_flags                : 10 ( 16.7%)\n",
            "      ip_flags                 :  4 (  6.7%)\n",
            "      address_diversity        :  6 ( 10.0%)\n",
            "      network_multiplexing     :  6 ( 10.0%)\n",
            "      timing_control           : 11 ( 18.3%)\n",
            "      fragmentation            :  2 (  3.3%)\n",
            "\n",
            " Tahapan 5 Complete!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tahapan 6: Data Augmentation (FIGS)"
      ],
      "metadata": {
        "id": "mzONOPfBtuqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.1 FIGAN for Limited Classes**"
      ],
      "metadata": {
        "id": "DBEPuCCJw21y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Lightweight Generator for FIGAN\"\"\"\n",
        "    def __init__(self, noise_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Discriminator for FIGAN\"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "MvpvELuZ7ahQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_figan(X_real, selected_features, noise_dim=100, epochs=50,\n",
        "                batch_size=256, save_every=10):\n",
        "    \"\"\"\n",
        "    Train FIGAN for generating synthetic samples\n",
        "    Only generates data for selected important features\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"      Using device: {device}\")\n",
        "\n",
        "    # Filter to selected features only\n",
        "    X_selected = X_real[selected_features].values\n",
        "    n_features = len(selected_features)\n",
        "\n",
        "    # Initialize models\n",
        "    generator = Generator(noise_dim, n_features).to(device)\n",
        "    discriminator = Discriminator(n_features).to(device)\n",
        "\n",
        "    # Optimizers\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Convert to tensor\n",
        "    X_tensor = torch.FloatTensor(X_selected).to(device)\n",
        "\n",
        "    # Training loop\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        discriminator.train()\n",
        "\n",
        "        # Real samples\n",
        "        real_samples = X_tensor[torch.randint(0, len(X_tensor), (batch_size,))]\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "        # Fake samples\n",
        "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "        fake_samples = generator(noise)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Discriminator loss\n",
        "        d_optimizer.zero_grad()\n",
        "\n",
        "        real_loss = criterion(discriminator(real_samples), real_labels)\n",
        "        fake_loss = criterion(discriminator(fake_samples.detach()), fake_labels)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        g_optimizer.zero_grad()\n",
        "\n",
        "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "        fake_samples = generator(noise)\n",
        "        g_loss = criterion(discriminator(fake_samples), real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Record losses\n",
        "        g_losses.append(g_loss.item())\n",
        "        d_losses.append(d_loss.item())\n",
        "\n",
        "        if (epoch + 1) % save_every == 0:\n",
        "            print(f\"         Epoch {epoch+1}/{epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    return generator, discriminator, g_losses, d_losses\n",
        "\n",
        "def generate_synthetic_samples(generator, n_samples, selected_features,\n",
        "                               full_feature_list, noise_dim=100):\n",
        "    \"\"\"\n",
        "    Generate synthetic samples and pad non-selected features with zeros\n",
        "    \"\"\"\n",
        "    device = next(generator.parameters()).device\n",
        "    generator.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        noise = torch.randn(n_samples, noise_dim).to(device)\n",
        "        synthetic_selected = generator(noise).cpu().numpy()\n",
        "\n",
        "    # Create full feature dataframe with zeros\n",
        "    synthetic_df = pd.DataFrame(\n",
        "        np.zeros((n_samples, len(full_feature_list))),\n",
        "        columns=full_feature_list\n",
        "    )\n",
        "\n",
        "    # Fill selected features with generated data\n",
        "    synthetic_df[selected_features] = synthetic_selected\n",
        "\n",
        "    return synthetic_df"
      ],
      "metadata": {
        "id": "78A2LFdt8aUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FIGAN for each Limited class\n",
        "print(\"Training FIGAN for Limited classes...\")\n",
        "\n",
        "synthetic_data_limited = {}\n",
        "\n",
        "for attack_class in limited_classes:\n",
        "    print(f\"\\n   Class: {attack_class}\")\n",
        "\n",
        "    # Get real samples\n",
        "    X_real = train_df[train_df['label2'] == attack_class][feature_cols]\n",
        "    n_real = len(X_real)\n",
        "\n",
        "    # Get selected features for this class\n",
        "    if attack_class in selected_features_by_class:\n",
        "        class_selected_features = consolidate_selected_features(\n",
        "            {attack_class: selected_features_by_class[attack_class]}\n",
        "        )\n",
        "    else:\n",
        "        # Fallback to all selected features\n",
        "        class_selected_features = selected_important_features\n",
        "\n",
        "    print(f\"      Using {len(class_selected_features)} selected features\")\n",
        "    print(f\"      Real samples: {n_real}\")\n",
        "\n",
        "    # Calculate target samples (balance to majority class)\n",
        "    n_majority = (train_df['label2'] == 'benign').sum()\n",
        "    n_target = int(n_majority * 0.5)  # Target 50% of majority\n",
        "    n_generate = max(0, n_target - n_real)\n",
        "\n",
        "    if n_generate == 0:\n",
        "        print(f\"      No generation needed (already sufficient)\")\n",
        "        continue\n",
        "\n",
        "    print(f\"      Target samples: {n_target}\")\n",
        "    print(f\"      Will generate: {n_generate} samples\")\n",
        "\n",
        "    # Train FIGAN\n",
        "    print(f\"      Training FIGAN...\")\n",
        "    generator, discriminator, g_losses, d_losses = train_figan(\n",
        "        X_real, class_selected_features,\n",
        "        noise_dim=100, epochs=50, batch_size=256\n",
        "    )\n",
        "\n",
        "    # Generate synthetic samples\n",
        "    print(f\"      Generating synthetic samples...\")\n",
        "    synthetic_df = generate_synthetic_samples(\n",
        "        generator, n_generate, class_selected_features,\n",
        "        feature_cols, noise_dim=100\n",
        "    )\n",
        "\n",
        "    synthetic_data_limited[attack_class] = synthetic_df\n",
        "\n",
        "    print(f\"       Generated {len(synthetic_df)} samples\")\n",
        "\n",
        "    # Clear memory\n",
        "    del generator, discriminator\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "Z2Y5l7_f8gWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f71dcf-b08c-4f26-aaf3-014b0df6e63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FIGAN for Limited classes...\n",
            "\n",
            "   Class: dos\n",
            "      Using 41 selected features\n",
            "      Real samples: 49841\n",
            "      Target samples: 125080\n",
            "      Will generate: 75239 samples\n",
            "      Training FIGAN...\n",
            "      Using device: cpu\n",
            "         Epoch 10/50 | D Loss: 0.6295 | G Loss: 0.6727\n",
            "         Epoch 20/50 | D Loss: 0.5902 | G Loss: 0.6459\n",
            "         Epoch 30/50 | D Loss: 0.5778 | G Loss: 0.6136\n",
            "         Epoch 40/50 | D Loss: 0.5690 | G Loss: 0.6378\n",
            "         Epoch 50/50 | D Loss: 0.5474 | G Loss: 0.6938\n",
            "      Generating synthetic samples...\n",
            "       Generated 75239 samples\n",
            "\n",
            "   Class: mitm\n",
            "      Using 41 selected features\n",
            "      Real samples: 21493\n",
            "      Target samples: 125080\n",
            "      Will generate: 103587 samples\n",
            "      Training FIGAN...\n",
            "      Using device: cpu\n",
            "         Epoch 10/50 | D Loss: 0.6494 | G Loss: 0.6896\n",
            "         Epoch 20/50 | D Loss: 0.6250 | G Loss: 0.6668\n",
            "         Epoch 30/50 | D Loss: 0.5949 | G Loss: 0.6883\n",
            "         Epoch 40/50 | D Loss: 0.5698 | G Loss: 0.7109\n",
            "         Epoch 50/50 | D Loss: 0.5784 | G Loss: 0.6926\n",
            "      Generating synthetic samples...\n",
            "       Generated 103587 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2 FISMOTE for Sparse Classes**\n"
      ],
      "metadata": {
        "id": "M-k62qWYw52K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fismote_generate(X_real, selected_features, full_feature_list,\n",
        "                     n_samples, k_neighbors=5):\n",
        "    \"\"\"\n",
        "    Feature-Importance SMOTE\n",
        "    Generate synthetic samples using SMOTE on selected features only\n",
        "    \"\"\"\n",
        "    print(f\" Using {len(selected_features)} selected features\")\n",
        "\n",
        "    # Extract selected features\n",
        "    X_selected = X_real[selected_features].values\n",
        "\n",
        "    if len(X_real) < k_neighbors:\n",
        "        k_neighbors = max(1, len(X_real) - 1)\n",
        "        print(f\"      Adjusted k_neighbors to {k_neighbors}\")\n",
        "\n",
        "    # Fit NearestNeighbors\n",
        "    nbrs = NearestNeighbors(n_neighbors=k_neighbors, algorithm='auto').fit(X_selected)\n",
        "\n",
        "    # Generate synthetic samples\n",
        "    synthetic_selected = []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Randomly select a sample\n",
        "        idx = np.random.randint(0, len(X_selected))\n",
        "        sample = X_selected[idx]\n",
        "\n",
        "        # Find k nearest neighbors\n",
        "        distances, indices = nbrs.kneighbors([sample])\n",
        "\n",
        "        # Randomly select a neighbor\n",
        "        neighbor_idx = np.random.choice(indices[0])\n",
        "        neighbor = X_selected[neighbor_idx]\n",
        "\n",
        "        # Interpolate\n",
        "        alpha = np.random.random()\n",
        "        synthetic_sample = sample + alpha * (neighbor - sample)\n",
        "        synthetic_selected.append(synthetic_sample)\n",
        "\n",
        "    synthetic_selected = np.array(synthetic_selected)\n",
        "\n",
        "    # Create full feature dataframe with zeros\n",
        "    synthetic_df = pd.DataFrame(\n",
        "        np.zeros((n_samples, len(full_feature_list))),\n",
        "        columns=full_feature_list\n",
        "    )\n",
        "\n",
        "    # Fill selected features\n",
        "    synthetic_df[selected_features] = synthetic_selected\n",
        "\n",
        "    return synthetic_df"
      ],
      "metadata": {
        "id": "sql1dPtF8H4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data for Sparse classes\n",
        "print(\"Generating synthetic data for Sparse classes...\")\n",
        "\n",
        "synthetic_data_sparse = {}\n",
        "\n",
        "for attack_class in sparse_classes:\n",
        "    print(f\"\\n   Class: {attack_class}\")\n",
        "\n",
        "    # Get real samples\n",
        "    X_real = train_df[train_df['label2'] == attack_class][feature_cols]\n",
        "    n_real = len(X_real)\n",
        "\n",
        "    # Get selected features\n",
        "    if attack_class in selected_features_by_class:\n",
        "        class_selected_features = consolidate_selected_features(\n",
        "            {attack_class: selected_features_by_class[attack_class]}\n",
        "        )\n",
        "    else:\n",
        "        class_selected_features = selected_important_features\n",
        "\n",
        "    print(f\"   Real samples: {n_real}\")\n",
        "    print(f\"   Using {len(class_selected_features)} selected features\")\n",
        "\n",
        "    # Calculate target samples\n",
        "    n_majority = (train_df['label2'] == 'benign').sum()\n",
        "    n_target = int(n_majority * 0.2)  # Target 20% of majority for sparse\n",
        "    n_generate = max(0, n_target - n_real)\n",
        "\n",
        "    if n_generate == 0:\n",
        "        print(f\"  No generation needed\")\n",
        "        continue\n",
        "\n",
        "    print(f\"  Target samples: {n_target}\")\n",
        "    print(f\"  Will generate: {n_generate} samples\")\n",
        "\n",
        "    # Generate using FISMOTE\n",
        "    print(f\"  Generating with FISMOTE...\")\n",
        "    synthetic_df = fismote_generate(\n",
        "        X_real, class_selected_features, feature_cols,\n",
        "        n_generate, k_neighbors=min(5, n_real-1)\n",
        "    )\n",
        "\n",
        "    synthetic_data_sparse[attack_class] = synthetic_df\n",
        "\n",
        "    print(f\" Generated {len(synthetic_df)} samples\")"
      ],
      "metadata": {
        "id": "fI1yXfn582Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e60f79-a14c-4595-e363-81c2dfea89f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic data for Sparse classes...\n",
            "\n",
            "   Class: malware\n",
            "   Real samples: 20747\n",
            "   Using 41 selected features\n",
            "  Target samples: 50032\n",
            "  Will generate: 29285 samples\n",
            "  Generating with FISMOTE...\n",
            " Using 41 selected features\n",
            " Generated 29285 samples\n",
            "\n",
            "   Class: web\n",
            "   Real samples: 7146\n",
            "   Using 41 selected features\n",
            "  Target samples: 50032\n",
            "  Will generate: 42886 samples\n",
            "  Generating with FISMOTE...\n",
            " Using 41 selected features\n",
            " Generated 42886 samples\n",
            "\n",
            "   Class: bruteforce\n",
            "   Real samples: 4619\n",
            "   Using 41 selected features\n",
            "  Target samples: 50032\n",
            "  Will generate: 45413 samples\n",
            "  Generating with FISMOTE...\n",
            " Using 41 selected features\n",
            " Generated 45413 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.3 Merge Augmented Data**"
      ],
      "metadata": {
        "id": "VC2497ufw8qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_augmented_data(train_df, synthetic_data_limited, synthetic_data_sparse,\n",
        "                        plentiful_classes, limited_classes, sparse_classes):\n",
        "    \"\"\"\n",
        "    Merge original and synthetic data\n",
        "    \"\"\"\n",
        "    augmented_dfs = []\n",
        "\n",
        "    # Add Plentiful classes (no augmentation)\n",
        "    for attack_class in plentiful_classes:\n",
        "        class_df = train_df[train_df['label2'] == attack_class].copy()\n",
        "        augmented_dfs.append(class_df)\n",
        "        print(f\"      {attack_class:12s} (Plentiful): {len(class_df):7,} samples (original)\")\n",
        "\n",
        "    # Add Limited classes (with FIGAN synthetic)\n",
        "    for attack_class in limited_classes:\n",
        "        class_df = train_df[train_df['label2'] == attack_class].copy()\n",
        "        original_count = len(class_df)\n",
        "\n",
        "        if attack_class in synthetic_data_limited:\n",
        "            synthetic_df = synthetic_data_limited[attack_class].copy()\n",
        "            synthetic_df['label2'] = attack_class\n",
        "            synthetic_df['time_window'] = class_df['time_window'].mode()[0]\n",
        "\n",
        "            augmented_dfs.append(class_df)\n",
        "            augmented_dfs.append(synthetic_df)\n",
        "\n",
        "            total_count = original_count + len(synthetic_df)\n",
        "            print(f\"      {attack_class:12s} (Limited):   {original_count:7,} + {len(synthetic_df):7,} = {total_count:7,}\")\n",
        "        else:\n",
        "            augmented_dfs.append(class_df)\n",
        "            print(f\"      {attack_class:12s} (Limited):   {original_count:7,} (no augmentation)\")\n",
        "\n",
        "    # Add Sparse classes (with FISMOTE synthetic)\n",
        "    for attack_class in sparse_classes:\n",
        "        class_df = train_df[train_df['label2'] == attack_class].copy()\n",
        "        original_count = len(class_df)\n",
        "\n",
        "        if attack_class in synthetic_data_sparse:\n",
        "            synthetic_df = synthetic_data_sparse[attack_class].copy()\n",
        "            synthetic_df['label2'] = attack_class\n",
        "            synthetic_df['time_window'] = class_df['time_window'].mode()[0]\n",
        "\n",
        "            augmented_dfs.append(class_df)\n",
        "            augmented_dfs.append(synthetic_df)\n",
        "\n",
        "            total_count = original_count + len(synthetic_df)\n",
        "            print(f\"      {attack_class:12s} (Sparse):    {original_count:7,} + {len(synthetic_df):7,} = {total_count:7,}\")\n",
        "        else:\n",
        "            augmented_dfs.append(class_df)\n",
        "            print(f\"      {attack_class:12s} (Sparse):    {original_count:7,} (no augmentation)\")\n",
        "\n",
        "    # Concatenate all\n",
        "    train_augmented = pd.concat(augmented_dfs, ignore_index=True)\n",
        "\n",
        "    return train_augmented"
      ],
      "metadata": {
        "id": "HwPxRGb07yWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge data\n",
        "print(\"Merging augmented data...\")\n",
        "train_augmented = merge_augmented_data(\n",
        "    train_df, synthetic_data_limited, synthetic_data_sparse,\n",
        "    plentiful_classes, limited_classes, sparse_classes\n",
        ")\n",
        "\n",
        "print(f\"\\n Augmentation Summary:\")\n",
        "print(f\"      Original train size: {len(train_df):,}\")\n",
        "print(f\"      Augmented train size: {len(train_augmented):,}\")\n",
        "print(f\"      Increase: {len(train_augmented) - len(train_df):,} samples ({((len(train_augmented)/len(train_df))-1)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "TbahoMdj9I3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64119d99-a32a-4b33-ecce-d3b9aac9d597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging augmented data...\n",
            "      benign       (Plentiful): 250,160 samples (original)\n",
            "      recon        (Plentiful):  84,591 samples (original)\n",
            "      ddos         (Plentiful):  47,529 samples (original)\n",
            "      dos          (Limited):    49,841 +  75,239 = 125,080\n",
            "      mitm         (Limited):    21,493 + 103,587 = 125,080\n",
            "      malware      (Sparse):     20,747 +  29,285 =  50,032\n",
            "      web          (Sparse):      7,146 +  42,886 =  50,032\n",
            "      bruteforce   (Sparse):      4,619 +  45,413 =  50,032\n",
            "\n",
            " Augmentation Summary:\n",
            "      Original train size: 486,126\n",
            "      Augmented train size: 782,536\n",
            "      Increase: 296,410 samples (61.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.4 Validate Synthetic Data Quality**"
      ],
      "metadata": {
        "id": "gNUBn2WZw-dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_synthetic_quality(train_original, train_augmented):\n",
        "    \"\"\"\n",
        "    Validate quality of synthetic data\n",
        "    \"\"\"\n",
        "    print(\"  Statistical Validation:\")\n",
        "\n",
        "    for attack_class in limited_classes + sparse_classes:\n",
        "        real_data = train_original[train_original['label2'] == attack_class][feature_cols]\n",
        "        aug_data = train_augmented[train_augmented['label2'] == attack_class][feature_cols]\n",
        "\n",
        "        if len(real_data) == len(aug_data):\n",
        "            continue  # No synthetic data generated\n",
        "\n",
        "        synthetic_data = aug_data.iloc[len(real_data):]  # Only synthetic\n",
        "\n",
        "        print(f\"\\n      {attack_class}:\")\n",
        "        print(f\"         Real samples: {len(real_data)}\")\n",
        "        print(f\"         Synthetic samples: {len(synthetic_data)}\")\n",
        "\n",
        "        # Compare distributions (first 5 features)\n",
        "        for feat in feature_cols[:5]:\n",
        "            real_mean = real_data[feat].mean()\n",
        "            synth_mean = synthetic_data[feat].mean()\n",
        "            diff_pct = abs((synth_mean - real_mean) / (real_mean + 1e-10)) * 100\n",
        "\n",
        "            print(f\"         {feat:40s}: Real={real_mean:7.3f}, Synth={synth_mean:7.3f}, Diff={diff_pct:5.1f}%\")\n",
        "\n",
        "# Validate\n",
        "validate_synthetic_quality(train_df, train_augmented)\n",
        "\n",
        "print(\"\\n Tahapan 6 Complete!\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "BH2ssgoi7lR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e63004d-a3f3-4160-8702-ea8c17a30bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Statistical Validation:\n",
            "\n",
            "      dos:\n",
            "         Real samples: 49841\n",
            "         Synthetic samples: 75239\n",
            "         log_data-ranges_avg                     : Real= -0.233, Synth=  0.000, Diff=100.0%\n",
            "         log_data-ranges_max                     : Real= -0.237, Synth= -0.408, Diff= 72.0%\n",
            "         log_data-ranges_min                     : Real= -0.228, Synth=  0.406, Diff=277.6%\n",
            "         log_data-ranges_std_deviation           : Real= -0.066, Synth= -0.337, Diff=409.5%\n",
            "         log_data-types_count                    : Real= -0.479, Synth= -0.554, Diff= 15.7%\n",
            "\n",
            "      mitm:\n",
            "         Real samples: 21493\n",
            "         Synthetic samples: 103587\n",
            "         log_data-ranges_avg                     : Real=  0.138, Synth= -0.222, Diff=260.8%\n",
            "         log_data-ranges_max                     : Real=  0.189, Synth=  0.000, Diff=100.0%\n",
            "         log_data-ranges_min                     : Real= -0.011, Synth= -0.084, Diff=679.0%\n",
            "         log_data-ranges_std_deviation           : Real=  0.757, Synth=  0.297, Diff= 60.8%\n",
            "         log_data-types_count                    : Real= -0.077, Synth=  0.268, Diff=447.1%\n",
            "\n",
            "      malware:\n",
            "         Real samples: 20747\n",
            "         Synthetic samples: 29285\n",
            "         log_data-ranges_avg                     : Real= -0.312, Synth= -0.311, Diff=  0.0%\n",
            "         log_data-ranges_max                     : Real= -0.315, Synth= -0.315, Diff=  0.0%\n",
            "         log_data-ranges_min                     : Real= -0.307, Synth= -0.307, Diff=  0.1%\n",
            "         log_data-ranges_std_deviation           : Real= -0.077, Synth= -0.077, Diff=  0.5%\n",
            "         log_data-types_count                    : Real= -0.461, Synth= -0.462, Diff=  0.1%\n",
            "\n",
            "      web:\n",
            "         Real samples: 7146\n",
            "         Synthetic samples: 42886\n",
            "         log_data-ranges_avg                     : Real= -0.316, Synth= -0.316, Diff=  0.0%\n",
            "         log_data-ranges_max                     : Real= -0.320, Synth= -0.320, Diff=  0.0%\n",
            "         log_data-ranges_min                     : Real= -0.311, Synth= -0.311, Diff=  0.0%\n",
            "         log_data-ranges_std_deviation           : Real= -0.085, Synth= -0.085, Diff=  0.0%\n",
            "         log_data-types_count                    : Real= -0.618, Synth= -0.618, Diff=  0.0%\n",
            "\n",
            "      bruteforce:\n",
            "         Real samples: 4619\n",
            "         Synthetic samples: 45413\n",
            "         log_data-ranges_avg                     : Real= -0.316, Synth= -0.316, Diff=  0.0%\n",
            "         log_data-ranges_max                     : Real= -0.320, Synth=  0.000, Diff=100.0%\n",
            "         log_data-ranges_min                     : Real= -0.311, Synth= -0.311, Diff=  0.0%\n",
            "         log_data-ranges_std_deviation           : Real= -0.085, Synth= -0.085, Diff=  0.0%\n",
            "         log_data-types_count                    : Real= -0.618, Synth= -0.618, Diff=  0.0%\n",
            "\n",
            " Tahapan 6 Complete!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tahapan 7: Model Training\n",
        "\n"
      ],
      "metadata": {
        "id": "z7NhyEKltwsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "print(\"Preparing training data...\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train_augmented = le.fit_transform(train_augmented['label2'])\n",
        "y_test = le.transform(test_df['label2'])\n",
        "\n",
        "X_train_augmented = train_augmented[feature_cols].values\n",
        "X_test = test_df[feature_cols].values\n",
        "\n",
        "print(f\"  X_train shape: {X_train_augmented.shape}\")\n",
        "print(f\"  X_test shape:  {X_test.shape}\")\n",
        "print(f\"  Classes: {le.classes_}\")"
      ],
      "metadata": {
        "id": "mEf4GR6UNmM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac501570-5176-482b-f60e-d44e9d38f7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training data...\n",
            "  X_train shape: (782536, 71)\n",
            "  X_test shape:  (60516, 71)\n",
            "  Classes: ['benign' 'bruteforce' 'ddos' 'dos' 'malware' 'mitm' 'recon' 'web']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.1 XGBoost Training**"
      ],
      "metadata": {
        "id": "HypYxPfQyP-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgboost(X_train, y_train, X_test, y_test, n_classes):\n",
        "    \"\"\"\n",
        "    Train XGBoost classifier with optimized parameters\n",
        "    \"\"\"\n",
        "    print(\"Initializing XGBoost...\")\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': n_classes,\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 100,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': 42,\n",
        "        'tree_method': 'hist',  # Faster for large datasets\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**xgb_params)\n",
        "\n",
        "    print(\"      Training XGBoost...\")\n",
        "    model.fit(X_train, y_train,\n",
        "             eval_set=[(X_test, y_test)],\n",
        "             verbose=False)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"      Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"      Test Accuracy:  {test_acc:.4f}\")\n",
        "\n",
        "    return model, y_pred_test"
      ],
      "metadata": {
        "id": "jGK4a20tR6m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost\n",
        "xgb_model, xgb_pred = train_xgboost(\n",
        "    X_train_augmented, y_train_augmented,\n",
        "    X_test, y_test, len(le.classes_)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al4EsJipSAF7",
        "outputId": "3c048f00-e37f-4818-aa9f-7b1f38466c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing XGBoost...\n",
            "      Training XGBoost...\n",
            "      Train Accuracy: 0.9832\n",
            "      Test Accuracy:  0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan Hasil Model\n",
        "joblib.dump(xgb_model, \"/content/drive/MyDrive/Dataset/CIC_IIoT_2025/Brave/xgb_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rKgLc2ESKEj",
        "outputId": "694899c8-9d12-4292-ba9f-404b8ac4cf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Dataset/CIC_IIoT_2025/Brave/xgb_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menload Kembali Hasil Model\n",
        "xgb_model = joblib.load(\"/content/drive/MyDrive/Dataset/CIC_IIoT_2025/Brave/xgb_model.pkl\")\n",
        "\n",
        "print(\"Model berhasil di load\")"
      ],
      "metadata": {
        "id": "JRAczdUITaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.2 LightGBM Training**\n"
      ],
      "metadata": {
        "id": "DYToN0jPygmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lightgbm(X_train, y_train, X_test, y_test, n_classes):\n",
        "    \"\"\"\n",
        "    Train LightGBM classifier with optimized parameters\n",
        "    \"\"\"\n",
        "    print(\"Initializing LightGBM...\")\n",
        "\n",
        "    lgb_params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': n_classes,\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 100,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    model = lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "    print(\"      Training LightGBM...\")\n",
        "    model.fit(X_train, y_train,\n",
        "             eval_set=[(X_test, y_test)],\n",
        "             eval_metric='multi_logloss',\n",
        "             callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)])\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"      Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"      Test Accuracy:  {test_acc:.4f}\")\n",
        "\n",
        "    return model, y_pred_test"
      ],
      "metadata": {
        "id": "xQ0VdfjrT6TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LightGBM\n",
        "lgb_model, lgb_pred = train_lightgbm(\n",
        "    X_train_augmented, y_train_augmented,\n",
        "    X_test, y_test, len(le.classes_)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niVpoeUxT-Ez",
        "outputId": "90d67c73-ea63-4609-9b45-093ec5727dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing LightGBM...\n",
            "      Training LightGBM...\n",
            "      Train Accuracy: 0.9864\n",
            "      Test Accuracy:  0.9866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan Hasil Model\n",
        "joblib.dump(lgb_model, \"/content/drive/MyDrive/Dataset/CIC_IIoT_2025/Brave/lgb_model.pkl\")\n",
        "\n",
        "print(\"Model berhasil disimpan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg7dsFmtU8bz",
        "outputId": "b43d69c8-6dcc-454f-fd40-00af2497a080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model berhasil disimpan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menload Kembali Hasil Model\n",
        "lgb_model = joblib.load(\"/content/drive/MyDrive/Dataset/CIC_IIoT_2025/Brave/lgb_model.pkl\")\n",
        "\n",
        "print(\"Model berhasil diload Kembali\")"
      ],
      "metadata": {
        "id": "7qYB1WOWVLUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.3 Random Forest Training**"
      ],
      "metadata": {
        "id": "jmB3-2FfyjEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_random_forest(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train Random Forest classifier\n",
        "    \"\"\"\n",
        "    print(\"Initializing Random Forest...\")\n",
        "\n",
        "    rf_params = {\n",
        "        'n_estimators': 100,\n",
        "        'max_depth': 20,\n",
        "        'min_samples_split': 5,\n",
        "        'min_samples_leaf': 2,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': 0\n",
        "    }\n",
        "\n",
        "    model = RandomForestClassifier(**rf_params)\n",
        "\n",
        "    print(\"      Training Random Forest...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"      Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"      Test Accuracy:  {test_acc:.4f}\")\n",
        "\n",
        "    return model, y_pred_test"
      ],
      "metadata": {
        "id": "on9Q5mZIVaXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest\n",
        "rf_model, rf_pred = train_random_forest(\n",
        "    X_train_augmented, y_train_augmented,\n",
        "    X_test, y_test\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSJGp2lWVdPa",
        "outputId": "d1e9396e-52fd-48d5-801b-8d3e7ce7c6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Random Forest...\n",
            "      Training Random Forest...\n",
            "      Train Accuracy: 0.9873\n",
            "      Test Accuracy:  0.9857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan Hasil Model\n",
        "joblib.dump(rf_model, \"/content/drive/MyDrive/Dataset/CIC_IIoT_2025/Brave/rf_model.pkl\")\n",
        "\n",
        "print(\"Model berhasil disimpan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHI18blfV1q7",
        "outputId": "27adc328-6d4e-4f1c-a045-ac5eeaf5f422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model berhasil disimpan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menload Kembali Hasil Model\n",
        "rf_model = joblib.load(\"/content/drive/MyDrive/Dataset/CIC_IIoT_2025/Brave/rf_model.pkl\")\n",
        "\n",
        "print(\"Model berhasil diload Kembali\")"
      ],
      "metadata": {
        "id": "suWU0EY7WC6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tahapan 7 Complete!\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PKnkAoAVz9a",
        "outputId": "1dc1cbba-5227-4d7a-d505-bd66b1dad9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tahapan 7 Complete!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "# Memori Tersedia\n",
        "memory_available = psutil.virtual_memory().available\n",
        "\n",
        "# Konversi ke GB\n",
        "memory_available_gb = memory_available / (1024 ** 3)\n",
        "\n",
        "print(f\"Jumlah memori yang tersedia: {memory_available_gb:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_nJWsuIXJ5j",
        "outputId": "200857e6-b7dc-455e-e70a-55607b95bf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah memori yang tersedia: 6.43 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tahapan 8: Evaluation"
      ],
      "metadata": {
        "id": "bPXGbmavtyvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.1 Predictions on Test Set**"
      ],
      "metadata": {
        "id": "yyvDFk-SzBer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_dict = {\n",
        "    'XGBoost': xgb_pred,\n",
        "    'LightGBM': lgb_pred,\n",
        "    'Random Forest': rf_pred\n",
        "}\n",
        "\n",
        "print(\"Predictions collected for all models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNJfGzSSY0i2",
        "outputId": "a4c5f62a-0f0f-4eb1-e6ee-852621bd3dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions collected for all models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.2 Metrics Calculation (per class)**"
      ],
      "metadata": {
        "id": "awaPWLWZzUD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics_per_class(y_true, y_pred, label_encoder):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive metrics for each class\n",
        "    \"\"\"\n",
        "    # Get class names\n",
        "    classes = label_encoder.classes_\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=range(len(classes)), zero_division=0\n",
        "    )\n",
        "\n",
        "    # Overall metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    # Macro/Weighted averages\n",
        "    macro_precision = precision.mean()\n",
        "    macro_recall = recall.mean()\n",
        "    macro_f1 = f1.mean()\n",
        "\n",
        "    weighted_precision = (precision * support).sum() / support.sum()\n",
        "    weighted_recall = (recall * support).sum() / support.sum()\n",
        "    weighted_f1 = (f1 * support).sum() / support.sum()\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Class': classes,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Support': support\n",
        "    })\n",
        "\n",
        "    # Add overall metrics\n",
        "    overall_metrics = {\n",
        "        'Accuracy': accuracy,\n",
        "        'MCC': mcc,\n",
        "        'Macro Precision': macro_precision,\n",
        "        'Macro Recall': macro_recall,\n",
        "        'Macro F1': macro_f1,\n",
        "        'Weighted Precision': weighted_precision,\n",
        "        'Weighted Recall': weighted_recall,\n",
        "        'Weighted F1': weighted_f1\n",
        "    }\n",
        "\n",
        "    return results_df, overall_metrics"
      ],
      "metadata": {
        "id": "VRamSeieZCpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics for all models\n",
        "all_results = {}\n",
        "\n",
        "for model_name, y_pred in predictions_dict.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "\n",
        "    results_df, overall_metrics = calculate_metrics_per_class(\n",
        "        y_test, y_pred, le\n",
        "    )\n",
        "\n",
        "    all_results[model_name] = {\n",
        "        'per_class': results_df,\n",
        "        'overall': overall_metrics\n",
        "    }\n",
        "\n",
        "    # Print per-class results\n",
        "    print(\"\\n      Per-Class Metrics:\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(\"\\n      Overall Metrics:\")\n",
        "    for metric, value in overall_metrics.items():\n",
        "        print(f\"         {metric:20s}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMCK4IJcZIqz",
        "outputId": "4cef9a47-1791-4271-943a-d5a962f90496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost:\n",
            "\n",
            "      Per-Class Metrics:\n",
            "     Class  Precision   Recall  F1-Score  Support\n",
            "    benign   0.972315 0.999391  0.985667    27903\n",
            "bruteforce   0.990000 0.977433  0.983676      709\n",
            "      ddos   0.997142 0.977131  0.987035     6428\n",
            "       dos   0.986559 0.989582  0.988068     6527\n",
            "   malware   0.991930 0.976849  0.984331     2894\n",
            "      mitm   0.985863 0.991201  0.988525     2955\n",
            "     recon   0.992228 0.944158  0.967596    12034\n",
            "       web   1.000000 0.954972  0.976967     1066\n",
            "\n",
            "      Overall Metrics:\n",
            "         Accuracy            : 0.9825\n",
            "         MCC                 : 0.9757\n",
            "         Macro Precision     : 0.9895\n",
            "         Macro Recall        : 0.9763\n",
            "         Macro F1            : 0.9827\n",
            "         Weighted Precision  : 0.9827\n",
            "         Weighted Recall     : 0.9825\n",
            "         Weighted F1         : 0.9824\n",
            "\n",
            "LightGBM:\n",
            "\n",
            "      Per-Class Metrics:\n",
            "     Class  Precision   Recall  F1-Score  Support\n",
            "    benign   0.977583 0.998674  0.988016    27903\n",
            "bruteforce   0.994326 0.988717  0.991513      709\n",
            "      ddos   0.995915 0.986154  0.991011     6428\n",
            "       dos   0.991585 0.992952  0.992268     6527\n",
            "   malware   0.997208 0.987215  0.992186     2894\n",
            "      mitm   0.994265 0.997293  0.995776     2955\n",
            "     recon   0.994619 0.952302  0.973001    12034\n",
            "       web   0.998102 0.986867  0.992453     1066\n",
            "\n",
            "      Overall Metrics:\n",
            "         Accuracy            : 0.9866\n",
            "         MCC                 : 0.9814\n",
            "         Macro Precision     : 0.9930\n",
            "         Macro Recall        : 0.9863\n",
            "         Macro F1            : 0.9895\n",
            "         Weighted Precision  : 0.9867\n",
            "         Weighted Recall     : 0.9866\n",
            "         Weighted F1         : 0.9865\n",
            "\n",
            "Random Forest:\n",
            "\n",
            "      Per-Class Metrics:\n",
            "     Class  Precision   Recall  F1-Score  Support\n",
            "    benign   0.972453 0.999462  0.985773    27903\n",
            "bruteforce   0.998538 0.963329  0.980617      709\n",
            "      ddos   0.999215 0.990666  0.994922     6428\n",
            "       dos   0.995094 0.994484  0.994789     6527\n",
            "   malware   0.996166 0.987560  0.991845     2894\n",
            "      mitm   0.998310 0.999323  0.998816     2955\n",
            "     recon   0.998072 0.946485  0.971594    12034\n",
            "       web   1.000000 0.955910  0.977458     1066\n",
            "\n",
            "      Overall Metrics:\n",
            "         Accuracy            : 0.9857\n",
            "         MCC                 : 0.9802\n",
            "         Macro Precision     : 0.9947\n",
            "         Macro Recall        : 0.9797\n",
            "         Macro F1            : 0.9870\n",
            "         Weighted Precision  : 0.9860\n",
            "         Weighted Recall     : 0.9857\n",
            "         Weighted F1         : 0.9856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.3 Confusion Matrix**"
      ],
      "metadata": {
        "id": "ohvOu5JHzeOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, label_encoder, model_name,\n",
        "                         save_path=None):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix heatmap\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    classes = label_encoder.classes_\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    # Add accuracy in title\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    plt.suptitle(f'Accuracy: {accuracy:.4f}', y=0.98, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"      Saved: {save_path}\")\n",
        "\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "Uq9e6QryaMKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrices for all models\n",
        "for model_name, y_pred in predictions_dict.items():\n",
        "    print(f\"\\n   Generating confusion matrix for {model_name}...\")\n",
        "    save_path = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
        "    plot_confusion_matrix(y_test, y_pred, le, model_name, save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OVRAUVbaPbL",
        "outputId": "fc8df6c7-7420-4fc1-d7d7-d0f3add7eb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   Generating confusion matrix for XGBoost...\n",
            "      Saved: confusion_matrix_xgboost.png\n",
            "\n",
            "   Generating confusion matrix for LightGBM...\n",
            "      Saved: confusion_matrix_lightgbm.png\n",
            "\n",
            "   Generating confusion matrix for Random Forest...\n",
            "      Saved: confusion_matrix_random_forest.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.4 Comparison with Baseline**"
      ],
      "metadata": {
        "id": "aYJcXVrRze9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_baseline_models(X_train_original, y_train_original, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train baseline models WITHOUT augmentation for comparison\n",
        "    \"\"\"\n",
        "    print(\"      Training baseline XGBoost (no augmentation)...\")\n",
        "\n",
        "    xgb_baseline = xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(np.unique(y_train_original)),\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        tree_method='hist',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    xgb_baseline.fit(X_train_original, y_train_original, verbose=False)\n",
        "    baseline_pred = xgb_baseline.predict(X_test)\n",
        "\n",
        "    return baseline_pred"
      ],
      "metadata": {
        "id": "ez_PWETPasuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare baseline data (original train without augmentation)\n",
        "X_train_original = train_df[feature_cols].values\n",
        "y_train_original = le.transform(train_df['label2'])"
      ],
      "metadata": {
        "id": "zdzAX4gYa1sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train baseline\n",
        "baseline_pred = train_baseline_models(\n",
        "    X_train_original, y_train_original, X_test, y_test\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QIdd8NMa5W7",
        "outputId": "e2c234f5-e1ea-4e9f-9945-fc218633c359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Training baseline XGBoost (no augmentation)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate baseline metrics\n",
        "print(\"Baseline Model (No Augmentation):\")\n",
        "baseline_results, baseline_overall = calculate_metrics_per_class(\n",
        "    y_test, baseline_pred, le\n",
        ")\n",
        "\n",
        "print(\"\\n      Per-Class Metrics:\")\n",
        "print(baseline_results.to_string(index=False))\n",
        "\n",
        "print(\"\\n      Overall Metrics:\")\n",
        "for metric, value in baseline_overall.items():\n",
        "    print(f\"         {metric:20s}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx9m3PW6bBCT",
        "outputId": "eb9f2081-57b0-4698-ad62-18f79bc46970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model (No Augmentation):\n",
            "\n",
            "      Per-Class Metrics:\n",
            "     Class  Precision   Recall  F1-Score  Support\n",
            "    benign   0.972648 0.999140  0.985716    27903\n",
            "bruteforce   0.995665 0.971791  0.983583      709\n",
            "      ddos   0.997147 0.978687  0.987831     6428\n",
            "       dos   0.987930 0.990654  0.989290     6527\n",
            "   malware   0.992665 0.982032  0.987320     2894\n",
            "      mitm   0.988467 0.986125  0.987295     2955\n",
            "     recon   0.990242 0.944491  0.966825    12034\n",
            "       web   1.000000 0.954972  0.976967     1066\n",
            "\n",
            "      Overall Metrics:\n",
            "         Accuracy            : 0.9826\n",
            "         MCC                 : 0.9759\n",
            "         Macro Precision     : 0.9906\n",
            "         Macro Recall        : 0.9760\n",
            "         Macro F1            : 0.9831\n",
            "         Weighted Precision  : 0.9829\n",
            "         Weighted Recall     : 0.9826\n",
            "         Weighted F1         : 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison table\n",
        "print(\"Performance Comparison (A-FIGS vs Baseline):\")\n",
        "print(\"\\n      Model Comparison (XGBoost):\")\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': list(baseline_overall.keys()),\n",
        "    'Baseline': list(baseline_overall.values()),\n",
        "    'A-FIGS (XGBoost)': list(all_results['XGBoost']['overall'].values())\n",
        "})\n",
        "\n",
        "comparison_df['Improvement'] = (\n",
        "    (comparison_df['A-FIGS (XGBoost)'] - comparison_df['Baseline']) /\n",
        "    comparison_df['Baseline'] * 100\n",
        ")\n",
        "\n",
        "print(comparison_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3e-tT39bOkj",
        "outputId": "e80e56ca-c5fa-46ab-a35f-9e323fd0fabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Comparison (A-FIGS vs Baseline):\n",
            "\n",
            "      Model Comparison (XGBoost):\n",
            "            Metric  Baseline  A-FIGS (XGBoost)  Improvement\n",
            "          Accuracy  0.982633          0.982467    -0.016817\n",
            "               MCC  0.975929          0.975709    -0.022591\n",
            "   Macro Precision  0.990595          0.989505    -0.110108\n",
            "      Macro Recall  0.975986          0.976340     0.036182\n",
            "          Macro F1  0.983103          0.982733    -0.037634\n",
            "Weighted Precision  0.982878          0.982743    -0.013781\n",
            "   Weighted Recall  0.982633          0.982467    -0.016817\n",
            "       Weighted F1  0.982544          0.982377    -0.017016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Per-class improvement\n",
        "print(\"Per-Class F1-Score Improvement:\")\n",
        "baseline_f1 = baseline_results.set_index('Class')['F1-Score']\n",
        "afigs_f1 = all_results['XGBoost']['per_class'].set_index('Class')['F1-Score']\n",
        "\n",
        "improvement_df = pd.DataFrame({\n",
        "    'Class': baseline_f1.index,\n",
        "    'Baseline F1': baseline_f1.values,\n",
        "    'A-FIGS F1': afigs_f1.values,\n",
        "    'Improvement': ((afigs_f1.values - baseline_f1.values) /\n",
        "                    (baseline_f1.values + 1e-10) * 100)\n",
        "})\n",
        "\n",
        "print(improvement_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3pQQnSfbR-E",
        "outputId": "ed753d46-2092-4f9d-8abb-08b0a8abc619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-Class F1-Score Improvement:\n",
            "     Class  Baseline F1  A-FIGS F1  Improvement\n",
            "    benign     0.985716   0.985667    -0.004943\n",
            "bruteforce     0.983583   0.983676     0.009477\n",
            "      ddos     0.987831   0.987035    -0.080509\n",
            "       dos     0.989290   0.988068    -0.123540\n",
            "   malware     0.987320   0.984331    -0.302669\n",
            "      mitm     0.987295   0.988525     0.124638\n",
            "     recon     0.966825   0.967596     0.079734\n",
            "       web     0.976967   0.976967     0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Highlight Limited and Sparse classes\n",
        "print(\"Focus on Augmented Classes:\")\n",
        "augmented_classes = limited_classes + sparse_classes\n",
        "for attack_class in augmented_classes:\n",
        "    baseline_f1_val = baseline_f1.get(attack_class, 0)\n",
        "    afigs_f1_val = afigs_f1.get(attack_class, 0)\n",
        "    improvement = ((afigs_f1_val - baseline_f1_val) / (baseline_f1_val + 1e-10) * 100)\n",
        "    category = categories.get(attack_class, 'Unknown')\n",
        "\n",
        "    print(f\"         {attack_class:12s} ({category:8s}): \"\n",
        "          f\"Baseline={baseline_f1_val:.4f}, A-FIGS={afigs_f1_val:.4f}, \"\n",
        "          f\"Improvement={improvement:+6.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_V064gmbeQ6",
        "outputId": "a019a5d2-75d0-4581-bfbd-819980252580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Focus on Augmented Classes:\n",
            "         dos          (Limited ): Baseline=0.9893, A-FIGS=0.9881, Improvement= -0.12%\n",
            "         mitm         (Limited ): Baseline=0.9873, A-FIGS=0.9885, Improvement= +0.12%\n",
            "         malware      (Sparse  ): Baseline=0.9873, A-FIGS=0.9843, Improvement= -0.30%\n",
            "         web          (Sparse  ): Baseline=0.9770, A-FIGS=0.9770, Improvement= +0.00%\n",
            "         bruteforce   (Sparse  ): Baseline=0.9836, A-FIGS=0.9837, Improvement= +0.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.5 Visualization & Reporting**"
      ],
      "metadata": {
        "id": "CiIORRCCzf7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_comparison(all_results, save_path='model_comparison.png'):\n",
        "    \"\"\"\n",
        "    Plot comprehensive model comparison\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    models = list(all_results.keys())\n",
        "\n",
        "    # 1. Overall Accuracy Comparison\n",
        "    accuracies = [all_results[model]['overall']['Accuracy'] for model in models]\n",
        "    axes[0, 0].bar(models, accuracies, color=['steelblue', 'coral', 'lightgreen'],\n",
        "                   edgecolor='black')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].set_title('Overall Accuracy Comparison')\n",
        "    axes[0, 0].set_ylim([0.7, 1.0])\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, v in enumerate(accuracies):\n",
        "        axes[0, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "    # 2. MCC Comparison\n",
        "    mccs = [all_results[model]['overall']['MCC'] for model in models]\n",
        "    axes[0, 1].bar(models, mccs, color=['steelblue', 'coral', 'lightgreen'],\n",
        "                   edgecolor='black')\n",
        "    axes[0, 1].set_ylabel('Matthews Correlation Coefficient')\n",
        "    axes[0, 1].set_title('MCC Comparison')\n",
        "    axes[0, 1].set_ylim([0.5, 1.0])\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, v in enumerate(mccs):\n",
        "        axes[0, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "    # 3. Macro F1-Score Comparison\n",
        "    macro_f1s = [all_results[model]['overall']['Macro F1'] for model in models]\n",
        "    axes[1, 0].bar(models, macro_f1s, color=['steelblue', 'coral', 'lightgreen'],\n",
        "                   edgecolor='black')\n",
        "    axes[1, 0].set_ylabel('Macro F1-Score')\n",
        "    axes[1, 0].set_title('Macro F1-Score Comparison')\n",
        "    axes[1, 0].set_ylim([0.7, 1.0])\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, v in enumerate(macro_f1s):\n",
        "        axes[1, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "    # 4. Per-Class F1-Score Comparison (Best model)\n",
        "    best_model = models[np.argmax(accuracies)]\n",
        "    per_class_f1 = all_results[best_model]['per_class']\n",
        "\n",
        "    colors = ['green' if cls in plentiful_classes\n",
        "              else 'orange' if cls in limited_classes\n",
        "              else 'red' for cls in per_class_f1['Class']]\n",
        "\n",
        "    axes[1, 1].barh(per_class_f1['Class'], per_class_f1['F1-Score'],\n",
        "                    color=colors, edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('F1-Score')\n",
        "    axes[1, 1].set_title(f'Per-Class F1-Score ({best_model})')\n",
        "    axes[1, 1].set_xlim([0, 1.1])\n",
        "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='green', edgecolor='black', label='Plentiful'),\n",
        "        Patch(facecolor='orange', edgecolor='black', label='Limited'),\n",
        "        Patch(facecolor='red', edgecolor='black', label='Sparse')\n",
        "    ]\n",
        "    axes[1, 1].legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"      Saved: {save_path}\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "L_fET_FmcFEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate comparison plot\n",
        "plot_model_comparison(all_results)\n",
        "\n",
        "def plot_baseline_vs_afigs(baseline_results, afigs_results, categories,\n",
        "                           save_path='baseline_vs_afigs.png'):\n",
        "    \"\"\"\n",
        "    Plot detailed baseline vs A-FIGS comparison\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    classes = baseline_results['Class'].values\n",
        "\n",
        "    # Get category colors\n",
        "    colors = ['green' if cls in plentiful_classes\n",
        "              else 'orange' if cls in limited_classes\n",
        "              else 'red' for cls in classes]\n",
        "\n",
        "    # 1. Precision Comparison\n",
        "    x = np.arange(len(classes))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[0].bar(x - width/2, baseline_results['Precision'], width,\n",
        "                label='Baseline', color='lightgray', edgecolor='black')\n",
        "    axes[0].bar(x + width/2, afigs_results['Precision'], width,\n",
        "                label='A-FIGS', color=colors, edgecolor='black', alpha=0.8)\n",
        "    axes[0].set_ylabel('Precision')\n",
        "    axes[0].set_title('Precision Comparison')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels(classes, rotation=45, ha='right')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 2. Recall Comparison\n",
        "    axes[1].bar(x - width/2, baseline_results['Recall'], width,\n",
        "                label='Baseline', color='lightgray', edgecolor='black')\n",
        "    axes[1].bar(x + width/2, afigs_results['Recall'], width,\n",
        "                label='A-FIGS', color=colors, edgecolor='black', alpha=0.8)\n",
        "    axes[1].set_ylabel('Recall')\n",
        "    axes[1].set_title('Recall Comparison')\n",
        "    axes[1].set_xticks(x)\n",
        "    axes[1].set_xticklabels(classes, rotation=45, ha='right')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 3. F1-Score Comparison\n",
        "    axes[2].bar(x - width/2, baseline_results['F1-Score'], width,\n",
        "                label='Baseline', color='lightgray', edgecolor='black')\n",
        "    axes[2].bar(x + width/2, afigs_results['F1-Score'], width,\n",
        "                label='A-FIGS', color=colors, edgecolor='black', alpha=0.8)\n",
        "    axes[2].set_ylabel('F1-Score')\n",
        "    axes[2].set_title('F1-Score Comparison')\n",
        "    axes[2].set_xticks(x)\n",
        "    axes[2].set_xticklabels(classes, rotation=45, ha='right')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add category legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='green', edgecolor='black', label='Plentiful'),\n",
        "        Patch(facecolor='orange', edgecolor='black', label='Limited'),\n",
        "        Patch(facecolor='red', edgecolor='black', label='Sparse')\n",
        "    ]\n",
        "    axes[2].legend(handles=legend_elements, loc='lower right',\n",
        "                   title='Category', framealpha=0.9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"      Saved: {save_path}\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdpxK6ICcN_D",
        "outputId": "2c64ab27-daf0-4ef2-ff5c-05cc67342470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Saved: model_comparison.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate baseline vs A-FIGS comparison\n",
        "plot_baseline_vs_afigs(\n",
        "    baseline_results,\n",
        "    all_results['XGBoost']['per_class'],\n",
        "    categories\n",
        ")\n",
        "\n",
        "# Generate final summary report\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. DATASET STATISTICS:\")\n",
        "print(f\"      Original Train: {len(train_df):,} samples\")\n",
        "print(f\"      Augmented Train: {len(train_augmented):,} samples (+{len(train_augmented)-len(train_df):,})\")\n",
        "print(f\"      Test Set: {len(test_df):,} samples\")\n",
        "print(f\"      Features: {len(feature_cols)}\")\n",
        "print(f\"      Selected Features: {len(selected_important_features)} ({(len(selected_important_features)/len(feature_cols)*100):.1f}%)\")\n",
        "\n",
        "print(\"\\n2. CLASS DISTRIBUTION:\")\n",
        "print(f\"      Plentiful: {len(plentiful_classes)} classes - {plentiful_classes}\")\n",
        "print(f\"      Limited:   {len(limited_classes)} classes - {limited_classes}\")\n",
        "print(f\"      Sparse:    {len(sparse_classes)} classes - {sparse_classes}\")\n",
        "\n",
        "print(\"\\n3. AUGMENTATION SUMMARY:\")\n",
        "total_synthetic = 0\n",
        "for attack_class in limited_classes:\n",
        "    if attack_class in synthetic_data_limited:\n",
        "        n_synth = len(synthetic_data_limited[attack_class])\n",
        "        total_synthetic += n_synth\n",
        "        print(f\"      {attack_class:12s} (FIGAN):   +{n_synth:,} samples\")\n",
        "\n",
        "for attack_class in sparse_classes:\n",
        "    if attack_class in synthetic_data_sparse:\n",
        "        n_synth = len(synthetic_data_sparse[attack_class])\n",
        "        total_synthetic += n_synth\n",
        "        print(f\"      {attack_class:12s} (FISMOTE): +{n_synth:,} samples\")\n",
        "\n",
        "print(f\"\\n      Total synthetic samples: {total_synthetic:,}\")\n",
        "\n",
        "print(\"\\n4. MODEL PERFORMANCE (Test Set):\")\n",
        "for model_name in ['XGBoost', 'LightGBM', 'Random Forest']:\n",
        "    metrics = all_results[model_name]['overall']\n",
        "    print(f\"\\n   {model_name}:\")\n",
        "    print(f\"      Accuracy:        {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"      MCC:             {metrics['MCC']:.4f}\")\n",
        "    print(f\"      Macro F1:        {metrics['Macro F1']:.4f}\")\n",
        "    print(f\"      Weighted F1:     {metrics['Weighted F1']:.4f}\")\n",
        "\n",
        "print(\"\\n5. A-FIGS IMPROVEMENT (vs Baseline):\")\n",
        "print(f\"      Accuracy:    {comparison_df.loc[comparison_df['Metric']=='Accuracy', 'Improvement'].values[0]:+.2f}%\")\n",
        "print(f\"      MCC:         {comparison_df.loc[comparison_df['Metric']=='MCC', 'Improvement'].values[0]:+.2f}%\")\n",
        "print(f\"      Macro F1:    {comparison_df.loc[comparison_df['Metric']=='Macro F1', 'Improvement'].values[0]:+.2f}%\")\n",
        "\n",
        "print(\"\\n6. CRITICAL ATTACK DETECTION (Sparse & Limited):\")\n",
        "for attack_class in augmented_classes:\n",
        "    if attack_class in baseline_f1.index and attack_class in afigs_f1.index:\n",
        "        baseline_val = baseline_f1[attack_class]\n",
        "        afigs_val = afigs_f1[attack_class]\n",
        "        improvement = ((afigs_val - baseline_val) / (baseline_val + 1e-10) * 100)\n",
        "        category = categories[attack_class]\n",
        "        severity = severity_weights[attack_class]\n",
        "\n",
        "        print(f\"      {attack_class:12s} | Severity={severity:.1f} | \"\n",
        "              f\"Baseline F1={baseline_val:.4f} | A-FIGS F1={afigs_val:.4f} | \"\n",
        "              f\"={improvement:+6.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCzfV0YEcXVT",
        "outputId": "69bbde5d-3504-4943-8f71-d466698f3c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Saved: baseline_vs_afigs.png\n",
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY REPORT\n",
            "================================================================================\n",
            "\n",
            "1. DATASET STATISTICS:\n",
            "      Original Train: 486,126 samples\n",
            "      Augmented Train: 782,536 samples (+296,410)\n",
            "      Test Set: 60,516 samples\n",
            "      Features: 71\n",
            "      Selected Features: 60 (84.5%)\n",
            "\n",
            "2. CLASS DISTRIBUTION:\n",
            "      Plentiful: 3 classes - ['benign', 'recon', 'ddos']\n",
            "      Limited:   2 classes - ['dos', 'mitm']\n",
            "      Sparse:    3 classes - ['malware', 'web', 'bruteforce']\n",
            "\n",
            "3. AUGMENTATION SUMMARY:\n",
            "      dos          (FIGAN):   +75,239 samples\n",
            "      mitm         (FIGAN):   +103,587 samples\n",
            "      malware      (FISMOTE): +29,285 samples\n",
            "      web          (FISMOTE): +42,886 samples\n",
            "      bruteforce   (FISMOTE): +45,413 samples\n",
            "\n",
            "      Total synthetic samples: 296,410\n",
            "\n",
            "4. MODEL PERFORMANCE (Test Set):\n",
            "\n",
            "   XGBoost:\n",
            "      Accuracy:        0.9825\n",
            "      MCC:             0.9757\n",
            "      Macro F1:        0.9827\n",
            "      Weighted F1:     0.9824\n",
            "\n",
            "   LightGBM:\n",
            "      Accuracy:        0.9866\n",
            "      MCC:             0.9814\n",
            "      Macro F1:        0.9895\n",
            "      Weighted F1:     0.9865\n",
            "\n",
            "   Random Forest:\n",
            "      Accuracy:        0.9857\n",
            "      MCC:             0.9802\n",
            "      Macro F1:        0.9870\n",
            "      Weighted F1:     0.9856\n",
            "\n",
            "5. A-FIGS IMPROVEMENT (vs Baseline):\n",
            "      Accuracy:    -0.02%\n",
            "      MCC:         -0.02%\n",
            "      Macro F1:    -0.04%\n",
            "\n",
            "6. CRITICAL ATTACK DETECTION (Sparse & Limited):\n",
            "      dos          | Severity=4.5 | Baseline F1=0.9893 | A-FIGS F1=0.9881 | = -0.12%\n",
            "      mitm         | Severity=4.5 | Baseline F1=0.9873 | A-FIGS F1=0.9885 | = +0.12%\n",
            "      malware      | Severity=5.0 | Baseline F1=0.9873 | A-FIGS F1=0.9843 | = -0.30%\n",
            "      web          | Severity=3.0 | Baseline F1=0.9770 | A-FIGS F1=0.9770 | = +0.00%\n",
            "      bruteforce   | Severity=3.5 | Baseline F1=0.9836 | A-FIGS F1=0.9837 | = +0.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memori Yang Tersedia\n",
        "memory_available = psutil.virtual_memory().available / (1024 ** 3)  # Convert to GB\n",
        "print(f\"Memory yang tersedia: {memory_available:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l_Z6-LCc_Dz",
        "outputId": "513b1b06-0d31-4d58-e63a-060d2c605bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory yang tersedia: 6.03 GB\n"
          ]
        }
      ]
    }
  ]
}